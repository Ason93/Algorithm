Jupyter Notebook
Part2-Training_InceptionV3-Student
(autosaved)
Current Kernel Logo
tf_training 
File
Edit
View
Insert
Cell
Kernel
Widgets
Help

Training the VMMR Cars Dataset with InceptionV3 on CPU
Copyright (c) 2019 Intel Corporation.

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

Objective
Understand the stages of preparing for training using the Keras with TensorFlow as a backend and the Inception v3 topology. You will initiate training and learn about the relationship between accuracy and loss. You will then learn about how to evalute your model, test out your model on a sample image and then freeze your model for use outside Keras.

Activities
In this section of the training you will

Create DataGenerator for your dataset
Learn about CPU Optimization for Tensorflow
Understand Hyperparameter Selection
Compile your model
Learn about callbacks
Start your training
Evaluate Your Model
Test Your Model on a sample image
Freeze your graph
As you follow this notebook, complete Activity sections to finish this workload.

Generate image data generators for training, validation and test data
Load Data

To ingest the data for training purposes we utilize the Keras ImageDataGenerator class. This allows us to easily read in a directory that is structured with each category in its respective folder. Earlier in the training during the Exploration phase we structured the data in this manner along with specific folders for train, test and validation. We're going to utilize a generator for each of those folder classes.

At this point we also are planning to use Inception V3 which has a Height and Width requirement of 299x299 so we instantiate that here so we can utilize it throughout the rest of the notebook. The generator will also resize images to that size before feeding it into training, testing or validation so we make sure it will work successfully.

Activity
In the cell below, change the WIDTH and HEIGHT values to match the size requirement for the Inception V3 topology 299x299. We're also going to utilize the same batch size for all three sets of data. Set the BATCH_SIZE to 64 then click Run.

from keras.preprocessing.image import ImageDataGenerator
from keras.applications.inception_v3 import preprocess_input, decode_predictions
​
WIDTH=299
HEIGHT=299
BATCH_SIZE=64
test_dir = 'test/'
train_dir = 'train/'
val_dir = 'val/'
​
#Train DataSet Generator with Augmentation
print("\nTraining Data Set")
train_generator = ImageDataGenerator(preprocessing_function=preprocess_input)
train_flow = train_generator.flow_from_directory(
    train_dir,
    target_size=(HEIGHT, WIDTH),
    batch_size = BATCH_SIZE
)
​
#Validation DataSet Generator with Augmentation
print("\nValidation Data Set")
val_generator = ImageDataGenerator(preprocessing_function=preprocess_input)
val_flow = val_generator.flow_from_directory(
    val_dir,
    target_size=(HEIGHT, WIDTH),
    batch_size = BATCH_SIZE
)
​
#Test DataSet Generator with Augmentation
print("\nTest Data Set")
test_generator = ImageDataGenerator(preprocessing_function=preprocess_input)
test_flow = test_generator.flow_from_directory(
    test_dir,
    target_size=(HEIGHT, WIDTH),
    batch_size = BATCH_SIZE
)
Using TensorFlow backend.

Training Data Set
Found 5098 images belonging to 10 classes.

Validation Data Set
Found 586 images belonging to 10 classes.

Test Data Set
Found 1193 images belonging to 10 classes.
Optimizations for CPU
CPU Optimization

CPUs, which includes Intel® Xeon processors, achieve optimal performance when TensorFlow is built from source with all of the instructions supported by the target CPU.

Beyond using the latest instruction sets, Intel has added support for the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) to TensorFlow. While the name is not completely accurate, these optimizations are often simply referred to as MKL or TensorFlow with MKL. TensorFlow with Intel MKL-DNN contains details on the Intel® MKL optimizations.

The two configurations listed below are used to optimize CPU performance by adjusting the thread pools.

intra_op_parallelism_threads: Nodes that can use multiple threads to parallelize their execution will schedule the individual pieces into this pool.
inter_op_parallelism_threads: All ready nodes are scheduled in this pool.
These configurations are set via the tf.ConfigProto and passed to tf.Session in the config attribute as shown in the snippet below. For both configuration options, if they are unset or set to zero, will default to the number of logical CPU cores. Testing has shown that the default is effective for systems ranging from one CPU with 4 cores to multiple CPUs with 70+ combined logical cores. A common alternative optimization is to set the number of threads in both pools equal to the number of physical cores rather than logical cores. Intel MKL uses the following environment variables to tune performance:

KMP_BLOCKTIME - Sets the time, in milliseconds, that a thread should wait, after completing the execution of a parallel region, before sleeping.
KMP_AFFINITY - Enables the runtime library to bind threads to physical processing units.
KMP_SETTINGS - Enables (true) or disables (false) the printing of OpenMP* runtime library environment variables during program execution.
OMP_NUM_THREADS - Specifies the number of threads to use.
See Optimizing for CPU, https://www.tensorflow.org/performance/performance_guide#optimizing_for_cpu

Activity
In the cell below, update NUM_PARALLEL_EXEC_UNITS to 8, KMP_BLOCKTIME to "1", and then click Run.

from keras.models import Sequential, Model, load_model
from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, CSVLogger
from keras import optimizers, models
from keras.layers import Dense, Dropout, GlobalAveragePooling2D
from keras import applications
from keras import backend as K
import tensorflow as tf
import os
​
NUM_PARALLEL_EXEC_UNITS = 16
​
#Set Performance Parameters for MKL and Tensorflow using Keras backend
#TensorFlow
config = tf.ConfigProto(
    intra_op_parallelism_threads=NUM_PARALLEL_EXEC_UNITS,
    inter_op_parallelism_threads=1
)
​
session = tf.Session(config=config)
K.set_session(session)
​
#MKL and OpenMP
os.environ["OMP_NUM_THREADS"] = str(NUM_PARALLEL_EXEC_UNITS)
os.environ["KMP_BLOCKTIME"] = "1"
os.environ["KMP_SETTINGS"] = "1"
os.environ["KMP_AFFINITY"]= "granularity=fine,verbose,compact,1,0"
Selecting Hyperparamaters - Recommendations
Hyperparameters

What batch size?
A batch size is the subset of the training dataset that is utilized in one iteration. It has been observed in practice that when using a larger batch there is a significant degradation in the quality of the model, as measured by its ability to generalize. The lack of generalization ability is due to the fact that large-batch methods tend to converge to sharp minimizers of the training function.

In general, batch size of 32 is a good starting point, and you should also try with 64, 128, and 256. Batch size below 32 might get too slow because of significantly lower computational speed as a result of not exploiting vectorization to the full extent.

https://arxiv.org/abs/1609.04836

What Learning rate to use? :
The Learning Rate is the size of the steps we take to reach a (local) minimum. A lower learning rate means more steps and therefore trains for a longer time while higher learning rate means less steps which therefore trains for shorter time. Also, too low a learning rate never progresses, and too high a learning rate causes instability and never converges. In between, there is a band of “just right” learning rates that successfully train.

There is no single learning rate that works for all optimizers. Learning rate can affect training time by an order of magnitude. Learning rate performance did not depend on model size. The same rates that performed best for 1x size performed best for 10x size.

Why and what optimizer to use?
Gradient Descent is one of the most popular algorithms to perform optimization and by far the most common way to optimize neural networks. Stochastic Gradient Descent(SGD) is a variant of Gradient Descent in which only a few of the samples (selected by batch_size) are used to compute gradient in every iteration. SGD can be optimized with parameter 'Momentum' which is a method that helps accelerate SGD in the relevant direction and dampens oscillations.

One of the challenges of SGD is that the same learning rate applies to all parameter updates. If the dataset is sparse and the features have very different frequencies, it will not be necessary to update all of them to the same extent, but rather a larger update for rarely occurring features. This problem is addressed by Adaptive Learning-Rate optimizers (Adagrad, Adadelta, RMSprop and Adam) that adapt the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters.

The main down side of the Adaptive Learning Rate optimizers is that they require more computation to be performed for each parameter in each training step and more state to be retained for each parameter. While a simple SGD Optimizer could equally be used with less computational requirements, it would require more hyperparameter tuning (learning rate) before it would converge as quickly.

In all, most of the optimizers manage to converge in a reasonable time.

https://arxiv.org/pdf/1609.04747.pdf

What is Transfer Learning?
Transfer learning is taking the weights from a previously trained network and use them as the basis for the weights in a new network. Since there is a difference in number of categories between data sets we normally remove the top layers of the network and re-instantiate them to match the number of categories we're trying to choose between. Using Transfer Learning will significantly speed up your training process by utilizing things like edge detection that the previous training has already learned then you can fine tune the network to your data set.

A commonly used transfer learning base is the ImageNet data set weights.

"What is ImageNet? ImageNet is an image dataset organized according to the WordNet hierarchy. Each meaningful concept in WordNet, possibly described by multiple words or word phrases, is called a "synonym set" or "synset". There are more than 100,000 synsets in WordNet, majority of them are nouns (80,000+). In ImageNet, we aim to provide on average 1000 images to illustrate each synset. Images of each concept are quality-controlled and human-annotated. In its completion, we hope ImageNet will offer tens of millions of cleanly sorted images for most of the concepts in the WordNet hierarchy."

http://image-net.org/about-overview

Initialize Training Top Layers
Start by brining in the pre-defined InceptionV3 network provided by Keras. We'll make sure to include the ImageNet weights since we want to utilize those weights for Transfer Learning which will speed up our training significantly. We'll also make sure the Top Layers aren't included since we don't want to predict 1001 classes and will then modify the network to fit our dataset.

Take the base model and add a GlobalAveragePooling2D layer and pass it the output of the base model. We'll then add a final Dense Layer or Fully Connected Layer that has a softmax activation which will do our predictions on the number of classes in our dataset. To make sure this is verstile we use the train_flow generator class indicies number so that it will automatically use the correct number of classes in the dataset.

Now we iterate over the initial layers of the base model and disable them for training by changing the layer.trainable variable to False. This means we'll only train over the new layers that we added specifically for our dataset.

Then compile your model and add the optimizer that you want to use. In this case we'll be using Adam with a Learning Rate of 0.001. We also want to use loss of Categorical Crossentropy since we have a multi-class classification problem.

We can print out the summary of the network after compiling so that we can verify the total number of nodes needing to be training and that the last layers were correctly added to the network.

Compile Model

Activity
In the cell below, update lr to 0.001, and then click Run.

# Initialize InceptionV3 with transfer learning
base_model = applications.InceptionV3(weights='imagenet', 
                                include_top=False, 
                                input_shape=(WIDTH, HEIGHT,3))
​
# add a global spatial average pooling layer
x = base_model.output
​
x = GlobalAveragePooling2D()(x)
# and a dense layer
x = Dense(1024, activation='relu')(x)
predictions = Dense(len(train_flow.class_indices), activation='softmax')(x)
​
# this is the model we will train
model = Model(inputs=base_model.input, outputs=predictions)
​
# first: train only the top layers (which were randomly initialized)
# i.e. freeze all convolutional InceptionV3 layers
for layer in base_model.layers:
    layer.trainable = False
​
# compile the model (should be done *after* setting layers to non-trainable)
model.compile(optimizer=optimizers.Adam(lr=0.001), metrics=['accuracy', 'top_k_categorical_accuracy'], loss='categorical_crossentropy')
model.summary()
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            (None, 299, 299, 3)  0                                            
__________________________________________________________________________________________________
conv2d_95 (Conv2D)              (None, 149, 149, 32) 864         input_2[0][0]                    
__________________________________________________________________________________________________
batch_normalization_95 (BatchNo (None, 149, 149, 32) 96          conv2d_95[0][0]                  
__________________________________________________________________________________________________
activation_95 (Activation)      (None, 149, 149, 32) 0           batch_normalization_95[0][0]     
__________________________________________________________________________________________________
conv2d_96 (Conv2D)              (None, 147, 147, 32) 9216        activation_95[0][0]              
__________________________________________________________________________________________________
batch_normalization_96 (BatchNo (None, 147, 147, 32) 96          conv2d_96[0][0]                  
__________________________________________________________________________________________________
activation_96 (Activation)      (None, 147, 147, 32) 0           batch_normalization_96[0][0]     
__________________________________________________________________________________________________
conv2d_97 (Conv2D)              (None, 147, 147, 64) 18432       activation_96[0][0]              
__________________________________________________________________________________________________
batch_normalization_97 (BatchNo (None, 147, 147, 64) 192         conv2d_97[0][0]                  
__________________________________________________________________________________________________
activation_97 (Activation)      (None, 147, 147, 64) 0           batch_normalization_97[0][0]     
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_97[0][0]              
__________________________________________________________________________________________________
conv2d_98 (Conv2D)              (None, 73, 73, 80)   5120        max_pooling2d_5[0][0]            
__________________________________________________________________________________________________
batch_normalization_98 (BatchNo (None, 73, 73, 80)   240         conv2d_98[0][0]                  
__________________________________________________________________________________________________
activation_98 (Activation)      (None, 73, 73, 80)   0           batch_normalization_98[0][0]     
__________________________________________________________________________________________________
conv2d_99 (Conv2D)              (None, 71, 71, 192)  138240      activation_98[0][0]              
__________________________________________________________________________________________________
batch_normalization_99 (BatchNo (None, 71, 71, 192)  576         conv2d_99[0][0]                  
__________________________________________________________________________________________________
activation_99 (Activation)      (None, 71, 71, 192)  0           batch_normalization_99[0][0]     
__________________________________________________________________________________________________
max_pooling2d_6 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_99[0][0]              
__________________________________________________________________________________________________
conv2d_103 (Conv2D)             (None, 35, 35, 64)   12288       max_pooling2d_6[0][0]            
__________________________________________________________________________________________________
batch_normalization_103 (BatchN (None, 35, 35, 64)   192         conv2d_103[0][0]                 
__________________________________________________________________________________________________
activation_103 (Activation)     (None, 35, 35, 64)   0           batch_normalization_103[0][0]    
__________________________________________________________________________________________________
conv2d_101 (Conv2D)             (None, 35, 35, 48)   9216        max_pooling2d_6[0][0]            
__________________________________________________________________________________________________
conv2d_104 (Conv2D)             (None, 35, 35, 96)   55296       activation_103[0][0]             
__________________________________________________________________________________________________
batch_normalization_101 (BatchN (None, 35, 35, 48)   144         conv2d_101[0][0]                 
__________________________________________________________________________________________________
batch_normalization_104 (BatchN (None, 35, 35, 96)   288         conv2d_104[0][0]                 
__________________________________________________________________________________________________
activation_101 (Activation)     (None, 35, 35, 48)   0           batch_normalization_101[0][0]    
__________________________________________________________________________________________________
activation_104 (Activation)     (None, 35, 35, 96)   0           batch_normalization_104[0][0]    
__________________________________________________________________________________________________
average_pooling2d_10 (AveragePo (None, 35, 35, 192)  0           max_pooling2d_6[0][0]            
__________________________________________________________________________________________________
conv2d_100 (Conv2D)             (None, 35, 35, 64)   12288       max_pooling2d_6[0][0]            
__________________________________________________________________________________________________
conv2d_102 (Conv2D)             (None, 35, 35, 64)   76800       activation_101[0][0]             
__________________________________________________________________________________________________
conv2d_105 (Conv2D)             (None, 35, 35, 96)   82944       activation_104[0][0]             
__________________________________________________________________________________________________
conv2d_106 (Conv2D)             (None, 35, 35, 32)   6144        average_pooling2d_10[0][0]       
__________________________________________________________________________________________________
batch_normalization_100 (BatchN (None, 35, 35, 64)   192         conv2d_100[0][0]                 
__________________________________________________________________________________________________
batch_normalization_102 (BatchN (None, 35, 35, 64)   192         conv2d_102[0][0]                 
__________________________________________________________________________________________________
batch_normalization_105 (BatchN (None, 35, 35, 96)   288         conv2d_105[0][0]                 
__________________________________________________________________________________________________
batch_normalization_106 (BatchN (None, 35, 35, 32)   96          conv2d_106[0][0]                 
__________________________________________________________________________________________________
activation_100 (Activation)     (None, 35, 35, 64)   0           batch_normalization_100[0][0]    
__________________________________________________________________________________________________
activation_102 (Activation)     (None, 35, 35, 64)   0           batch_normalization_102[0][0]    
__________________________________________________________________________________________________
activation_105 (Activation)     (None, 35, 35, 96)   0           batch_normalization_105[0][0]    
__________________________________________________________________________________________________
activation_106 (Activation)     (None, 35, 35, 32)   0           batch_normalization_106[0][0]    
__________________________________________________________________________________________________
mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_100[0][0]             
                                                                 activation_102[0][0]             
                                                                 activation_105[0][0]             
                                                                 activation_106[0][0]             
__________________________________________________________________________________________________
conv2d_110 (Conv2D)             (None, 35, 35, 64)   16384       mixed0[0][0]                     
__________________________________________________________________________________________________
batch_normalization_110 (BatchN (None, 35, 35, 64)   192         conv2d_110[0][0]                 
__________________________________________________________________________________________________
activation_110 (Activation)     (None, 35, 35, 64)   0           batch_normalization_110[0][0]    
__________________________________________________________________________________________________
conv2d_108 (Conv2D)             (None, 35, 35, 48)   12288       mixed0[0][0]                     
__________________________________________________________________________________________________
conv2d_111 (Conv2D)             (None, 35, 35, 96)   55296       activation_110[0][0]             
__________________________________________________________________________________________________
batch_normalization_108 (BatchN (None, 35, 35, 48)   144         conv2d_108[0][0]                 
__________________________________________________________________________________________________
batch_normalization_111 (BatchN (None, 35, 35, 96)   288         conv2d_111[0][0]                 
__________________________________________________________________________________________________
activation_108 (Activation)     (None, 35, 35, 48)   0           batch_normalization_108[0][0]    
__________________________________________________________________________________________________
activation_111 (Activation)     (None, 35, 35, 96)   0           batch_normalization_111[0][0]    
__________________________________________________________________________________________________
average_pooling2d_11 (AveragePo (None, 35, 35, 256)  0           mixed0[0][0]                     
__________________________________________________________________________________________________
conv2d_107 (Conv2D)             (None, 35, 35, 64)   16384       mixed0[0][0]                     
__________________________________________________________________________________________________
conv2d_109 (Conv2D)             (None, 35, 35, 64)   76800       activation_108[0][0]             
__________________________________________________________________________________________________
conv2d_112 (Conv2D)             (None, 35, 35, 96)   82944       activation_111[0][0]             
__________________________________________________________________________________________________
conv2d_113 (Conv2D)             (None, 35, 35, 64)   16384       average_pooling2d_11[0][0]       
__________________________________________________________________________________________________
batch_normalization_107 (BatchN (None, 35, 35, 64)   192         conv2d_107[0][0]                 
__________________________________________________________________________________________________
batch_normalization_109 (BatchN (None, 35, 35, 64)   192         conv2d_109[0][0]                 
__________________________________________________________________________________________________
batch_normalization_112 (BatchN (None, 35, 35, 96)   288         conv2d_112[0][0]                 
__________________________________________________________________________________________________
batch_normalization_113 (BatchN (None, 35, 35, 64)   192         conv2d_113[0][0]                 
__________________________________________________________________________________________________
activation_107 (Activation)     (None, 35, 35, 64)   0           batch_normalization_107[0][0]    
__________________________________________________________________________________________________
activation_109 (Activation)     (None, 35, 35, 64)   0           batch_normalization_109[0][0]    
__________________________________________________________________________________________________
activation_112 (Activation)     (None, 35, 35, 96)   0           batch_normalization_112[0][0]    
__________________________________________________________________________________________________
activation_113 (Activation)     (None, 35, 35, 64)   0           batch_normalization_113[0][0]    
__________________________________________________________________________________________________
mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_107[0][0]             
                                                                 activation_109[0][0]             
                                                                 activation_112[0][0]             
                                                                 activation_113[0][0]             
__________________________________________________________________________________________________
conv2d_117 (Conv2D)             (None, 35, 35, 64)   18432       mixed1[0][0]                     
__________________________________________________________________________________________________
batch_normalization_117 (BatchN (None, 35, 35, 64)   192         conv2d_117[0][0]                 
__________________________________________________________________________________________________
activation_117 (Activation)     (None, 35, 35, 64)   0           batch_normalization_117[0][0]    
__________________________________________________________________________________________________
conv2d_115 (Conv2D)             (None, 35, 35, 48)   13824       mixed1[0][0]                     
__________________________________________________________________________________________________
conv2d_118 (Conv2D)             (None, 35, 35, 96)   55296       activation_117[0][0]             
__________________________________________________________________________________________________
batch_normalization_115 (BatchN (None, 35, 35, 48)   144         conv2d_115[0][0]                 
__________________________________________________________________________________________________
batch_normalization_118 (BatchN (None, 35, 35, 96)   288         conv2d_118[0][0]                 
__________________________________________________________________________________________________
activation_115 (Activation)     (None, 35, 35, 48)   0           batch_normalization_115[0][0]    
__________________________________________________________________________________________________
activation_118 (Activation)     (None, 35, 35, 96)   0           batch_normalization_118[0][0]    
__________________________________________________________________________________________________
average_pooling2d_12 (AveragePo (None, 35, 35, 288)  0           mixed1[0][0]                     
__________________________________________________________________________________________________
conv2d_114 (Conv2D)             (None, 35, 35, 64)   18432       mixed1[0][0]                     
__________________________________________________________________________________________________
conv2d_116 (Conv2D)             (None, 35, 35, 64)   76800       activation_115[0][0]             
__________________________________________________________________________________________________
conv2d_119 (Conv2D)             (None, 35, 35, 96)   82944       activation_118[0][0]             
__________________________________________________________________________________________________
conv2d_120 (Conv2D)             (None, 35, 35, 64)   18432       average_pooling2d_12[0][0]       
__________________________________________________________________________________________________
batch_normalization_114 (BatchN (None, 35, 35, 64)   192         conv2d_114[0][0]                 
__________________________________________________________________________________________________
batch_normalization_116 (BatchN (None, 35, 35, 64)   192         conv2d_116[0][0]                 
__________________________________________________________________________________________________
batch_normalization_119 (BatchN (None, 35, 35, 96)   288         conv2d_119[0][0]                 
__________________________________________________________________________________________________
batch_normalization_120 (BatchN (None, 35, 35, 64)   192         conv2d_120[0][0]                 
__________________________________________________________________________________________________
activation_114 (Activation)     (None, 35, 35, 64)   0           batch_normalization_114[0][0]    
__________________________________________________________________________________________________
activation_116 (Activation)     (None, 35, 35, 64)   0           batch_normalization_116[0][0]    
__________________________________________________________________________________________________
activation_119 (Activation)     (None, 35, 35, 96)   0           batch_normalization_119[0][0]    
__________________________________________________________________________________________________
activation_120 (Activation)     (None, 35, 35, 64)   0           batch_normalization_120[0][0]    
__________________________________________________________________________________________________
mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_114[0][0]             
                                                                 activation_116[0][0]             
                                                                 activation_119[0][0]             
                                                                 activation_120[0][0]             
__________________________________________________________________________________________________
conv2d_122 (Conv2D)             (None, 35, 35, 64)   18432       mixed2[0][0]                     
__________________________________________________________________________________________________
batch_normalization_122 (BatchN (None, 35, 35, 64)   192         conv2d_122[0][0]                 
__________________________________________________________________________________________________
activation_122 (Activation)     (None, 35, 35, 64)   0           batch_normalization_122[0][0]    
__________________________________________________________________________________________________
conv2d_123 (Conv2D)             (None, 35, 35, 96)   55296       activation_122[0][0]             
__________________________________________________________________________________________________
batch_normalization_123 (BatchN (None, 35, 35, 96)   288         conv2d_123[0][0]                 
__________________________________________________________________________________________________
activation_123 (Activation)     (None, 35, 35, 96)   0           batch_normalization_123[0][0]    
__________________________________________________________________________________________________
conv2d_121 (Conv2D)             (None, 17, 17, 384)  995328      mixed2[0][0]                     
__________________________________________________________________________________________________
conv2d_124 (Conv2D)             (None, 17, 17, 96)   82944       activation_123[0][0]             
__________________________________________________________________________________________________
batch_normalization_121 (BatchN (None, 17, 17, 384)  1152        conv2d_121[0][0]                 
__________________________________________________________________________________________________
batch_normalization_124 (BatchN (None, 17, 17, 96)   288         conv2d_124[0][0]                 
__________________________________________________________________________________________________
activation_121 (Activation)     (None, 17, 17, 384)  0           batch_normalization_121[0][0]    
__________________________________________________________________________________________________
activation_124 (Activation)     (None, 17, 17, 96)   0           batch_normalization_124[0][0]    
__________________________________________________________________________________________________
max_pooling2d_7 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]                     
__________________________________________________________________________________________________
mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_121[0][0]             
                                                                 activation_124[0][0]             
                                                                 max_pooling2d_7[0][0]            
__________________________________________________________________________________________________
conv2d_129 (Conv2D)             (None, 17, 17, 128)  98304       mixed3[0][0]                     
__________________________________________________________________________________________________
batch_normalization_129 (BatchN (None, 17, 17, 128)  384         conv2d_129[0][0]                 
__________________________________________________________________________________________________
activation_129 (Activation)     (None, 17, 17, 128)  0           batch_normalization_129[0][0]    
__________________________________________________________________________________________________
conv2d_130 (Conv2D)             (None, 17, 17, 128)  114688      activation_129[0][0]             
__________________________________________________________________________________________________
batch_normalization_130 (BatchN (None, 17, 17, 128)  384         conv2d_130[0][0]                 
__________________________________________________________________________________________________
activation_130 (Activation)     (None, 17, 17, 128)  0           batch_normalization_130[0][0]    
__________________________________________________________________________________________________
conv2d_126 (Conv2D)             (None, 17, 17, 128)  98304       mixed3[0][0]                     
__________________________________________________________________________________________________
conv2d_131 (Conv2D)             (None, 17, 17, 128)  114688      activation_130[0][0]             
__________________________________________________________________________________________________
batch_normalization_126 (BatchN (None, 17, 17, 128)  384         conv2d_126[0][0]                 
__________________________________________________________________________________________________
batch_normalization_131 (BatchN (None, 17, 17, 128)  384         conv2d_131[0][0]                 
__________________________________________________________________________________________________
activation_126 (Activation)     (None, 17, 17, 128)  0           batch_normalization_126[0][0]    
__________________________________________________________________________________________________
activation_131 (Activation)     (None, 17, 17, 128)  0           batch_normalization_131[0][0]    
__________________________________________________________________________________________________
conv2d_127 (Conv2D)             (None, 17, 17, 128)  114688      activation_126[0][0]             
__________________________________________________________________________________________________
conv2d_132 (Conv2D)             (None, 17, 17, 128)  114688      activation_131[0][0]             
__________________________________________________________________________________________________
batch_normalization_127 (BatchN (None, 17, 17, 128)  384         conv2d_127[0][0]                 
__________________________________________________________________________________________________
batch_normalization_132 (BatchN (None, 17, 17, 128)  384         conv2d_132[0][0]                 
__________________________________________________________________________________________________
activation_127 (Activation)     (None, 17, 17, 128)  0           batch_normalization_127[0][0]    
__________________________________________________________________________________________________
activation_132 (Activation)     (None, 17, 17, 128)  0           batch_normalization_132[0][0]    
__________________________________________________________________________________________________
average_pooling2d_13 (AveragePo (None, 17, 17, 768)  0           mixed3[0][0]                     
__________________________________________________________________________________________________
conv2d_125 (Conv2D)             (None, 17, 17, 192)  147456      mixed3[0][0]                     
__________________________________________________________________________________________________
conv2d_128 (Conv2D)             (None, 17, 17, 192)  172032      activation_127[0][0]             
__________________________________________________________________________________________________
conv2d_133 (Conv2D)             (None, 17, 17, 192)  172032      activation_132[0][0]             
__________________________________________________________________________________________________
conv2d_134 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_13[0][0]       
__________________________________________________________________________________________________
batch_normalization_125 (BatchN (None, 17, 17, 192)  576         conv2d_125[0][0]                 
__________________________________________________________________________________________________
batch_normalization_128 (BatchN (None, 17, 17, 192)  576         conv2d_128[0][0]                 
__________________________________________________________________________________________________
batch_normalization_133 (BatchN (None, 17, 17, 192)  576         conv2d_133[0][0]                 
__________________________________________________________________________________________________
batch_normalization_134 (BatchN (None, 17, 17, 192)  576         conv2d_134[0][0]                 
__________________________________________________________________________________________________
activation_125 (Activation)     (None, 17, 17, 192)  0           batch_normalization_125[0][0]    
__________________________________________________________________________________________________
activation_128 (Activation)     (None, 17, 17, 192)  0           batch_normalization_128[0][0]    
__________________________________________________________________________________________________
activation_133 (Activation)     (None, 17, 17, 192)  0           batch_normalization_133[0][0]    
__________________________________________________________________________________________________
activation_134 (Activation)     (None, 17, 17, 192)  0           batch_normalization_134[0][0]    
__________________________________________________________________________________________________
mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_125[0][0]             
                                                                 activation_128[0][0]             
                                                                 activation_133[0][0]             
                                                                 activation_134[0][0]             
__________________________________________________________________________________________________
conv2d_139 (Conv2D)             (None, 17, 17, 160)  122880      mixed4[0][0]                     
__________________________________________________________________________________________________
batch_normalization_139 (BatchN (None, 17, 17, 160)  480         conv2d_139[0][0]                 
__________________________________________________________________________________________________
activation_139 (Activation)     (None, 17, 17, 160)  0           batch_normalization_139[0][0]    
__________________________________________________________________________________________________
conv2d_140 (Conv2D)             (None, 17, 17, 160)  179200      activation_139[0][0]             
__________________________________________________________________________________________________
batch_normalization_140 (BatchN (None, 17, 17, 160)  480         conv2d_140[0][0]                 
__________________________________________________________________________________________________
activation_140 (Activation)     (None, 17, 17, 160)  0           batch_normalization_140[0][0]    
__________________________________________________________________________________________________
conv2d_136 (Conv2D)             (None, 17, 17, 160)  122880      mixed4[0][0]                     
__________________________________________________________________________________________________
conv2d_141 (Conv2D)             (None, 17, 17, 160)  179200      activation_140[0][0]             
__________________________________________________________________________________________________
batch_normalization_136 (BatchN (None, 17, 17, 160)  480         conv2d_136[0][0]                 
__________________________________________________________________________________________________
batch_normalization_141 (BatchN (None, 17, 17, 160)  480         conv2d_141[0][0]                 
__________________________________________________________________________________________________
activation_136 (Activation)     (None, 17, 17, 160)  0           batch_normalization_136[0][0]    
__________________________________________________________________________________________________
activation_141 (Activation)     (None, 17, 17, 160)  0           batch_normalization_141[0][0]    
__________________________________________________________________________________________________
conv2d_137 (Conv2D)             (None, 17, 17, 160)  179200      activation_136[0][0]             
__________________________________________________________________________________________________
conv2d_142 (Conv2D)             (None, 17, 17, 160)  179200      activation_141[0][0]             
__________________________________________________________________________________________________
batch_normalization_137 (BatchN (None, 17, 17, 160)  480         conv2d_137[0][0]                 
__________________________________________________________________________________________________
batch_normalization_142 (BatchN (None, 17, 17, 160)  480         conv2d_142[0][0]                 
__________________________________________________________________________________________________
activation_137 (Activation)     (None, 17, 17, 160)  0           batch_normalization_137[0][0]    
__________________________________________________________________________________________________
activation_142 (Activation)     (None, 17, 17, 160)  0           batch_normalization_142[0][0]    
__________________________________________________________________________________________________
average_pooling2d_14 (AveragePo (None, 17, 17, 768)  0           mixed4[0][0]                     
__________________________________________________________________________________________________
conv2d_135 (Conv2D)             (None, 17, 17, 192)  147456      mixed4[0][0]                     
__________________________________________________________________________________________________
conv2d_138 (Conv2D)             (None, 17, 17, 192)  215040      activation_137[0][0]             
__________________________________________________________________________________________________
conv2d_143 (Conv2D)             (None, 17, 17, 192)  215040      activation_142[0][0]             
__________________________________________________________________________________________________
conv2d_144 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_14[0][0]       
__________________________________________________________________________________________________
batch_normalization_135 (BatchN (None, 17, 17, 192)  576         conv2d_135[0][0]                 
__________________________________________________________________________________________________
batch_normalization_138 (BatchN (None, 17, 17, 192)  576         conv2d_138[0][0]                 
__________________________________________________________________________________________________
batch_normalization_143 (BatchN (None, 17, 17, 192)  576         conv2d_143[0][0]                 
__________________________________________________________________________________________________
batch_normalization_144 (BatchN (None, 17, 17, 192)  576         conv2d_144[0][0]                 
__________________________________________________________________________________________________
activation_135 (Activation)     (None, 17, 17, 192)  0           batch_normalization_135[0][0]    
__________________________________________________________________________________________________
activation_138 (Activation)     (None, 17, 17, 192)  0           batch_normalization_138[0][0]    
__________________________________________________________________________________________________
activation_143 (Activation)     (None, 17, 17, 192)  0           batch_normalization_143[0][0]    
__________________________________________________________________________________________________
activation_144 (Activation)     (None, 17, 17, 192)  0           batch_normalization_144[0][0]    
__________________________________________________________________________________________________
mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_135[0][0]             
                                                                 activation_138[0][0]             
                                                                 activation_143[0][0]             
                                                                 activation_144[0][0]             
__________________________________________________________________________________________________
conv2d_149 (Conv2D)             (None, 17, 17, 160)  122880      mixed5[0][0]                     
__________________________________________________________________________________________________
batch_normalization_149 (BatchN (None, 17, 17, 160)  480         conv2d_149[0][0]                 
__________________________________________________________________________________________________
activation_149 (Activation)     (None, 17, 17, 160)  0           batch_normalization_149[0][0]    
__________________________________________________________________________________________________
conv2d_150 (Conv2D)             (None, 17, 17, 160)  179200      activation_149[0][0]             
__________________________________________________________________________________________________
batch_normalization_150 (BatchN (None, 17, 17, 160)  480         conv2d_150[0][0]                 
__________________________________________________________________________________________________
activation_150 (Activation)     (None, 17, 17, 160)  0           batch_normalization_150[0][0]    
__________________________________________________________________________________________________
conv2d_146 (Conv2D)             (None, 17, 17, 160)  122880      mixed5[0][0]                     
__________________________________________________________________________________________________
conv2d_151 (Conv2D)             (None, 17, 17, 160)  179200      activation_150[0][0]             
__________________________________________________________________________________________________
batch_normalization_146 (BatchN (None, 17, 17, 160)  480         conv2d_146[0][0]                 
__________________________________________________________________________________________________
batch_normalization_151 (BatchN (None, 17, 17, 160)  480         conv2d_151[0][0]                 
__________________________________________________________________________________________________
activation_146 (Activation)     (None, 17, 17, 160)  0           batch_normalization_146[0][0]    
__________________________________________________________________________________________________
activation_151 (Activation)     (None, 17, 17, 160)  0           batch_normalization_151[0][0]    
__________________________________________________________________________________________________
conv2d_147 (Conv2D)             (None, 17, 17, 160)  179200      activation_146[0][0]             
__________________________________________________________________________________________________
conv2d_152 (Conv2D)             (None, 17, 17, 160)  179200      activation_151[0][0]             
__________________________________________________________________________________________________
batch_normalization_147 (BatchN (None, 17, 17, 160)  480         conv2d_147[0][0]                 
__________________________________________________________________________________________________
batch_normalization_152 (BatchN (None, 17, 17, 160)  480         conv2d_152[0][0]                 
__________________________________________________________________________________________________
activation_147 (Activation)     (None, 17, 17, 160)  0           batch_normalization_147[0][0]    
__________________________________________________________________________________________________
activation_152 (Activation)     (None, 17, 17, 160)  0           batch_normalization_152[0][0]    
__________________________________________________________________________________________________
average_pooling2d_15 (AveragePo (None, 17, 17, 768)  0           mixed5[0][0]                     
__________________________________________________________________________________________________
conv2d_145 (Conv2D)             (None, 17, 17, 192)  147456      mixed5[0][0]                     
__________________________________________________________________________________________________
conv2d_148 (Conv2D)             (None, 17, 17, 192)  215040      activation_147[0][0]             
__________________________________________________________________________________________________
conv2d_153 (Conv2D)             (None, 17, 17, 192)  215040      activation_152[0][0]             
__________________________________________________________________________________________________
conv2d_154 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_15[0][0]       
__________________________________________________________________________________________________
batch_normalization_145 (BatchN (None, 17, 17, 192)  576         conv2d_145[0][0]                 
__________________________________________________________________________________________________
batch_normalization_148 (BatchN (None, 17, 17, 192)  576         conv2d_148[0][0]                 
__________________________________________________________________________________________________
batch_normalization_153 (BatchN (None, 17, 17, 192)  576         conv2d_153[0][0]                 
__________________________________________________________________________________________________
batch_normalization_154 (BatchN (None, 17, 17, 192)  576         conv2d_154[0][0]                 
__________________________________________________________________________________________________
activation_145 (Activation)     (None, 17, 17, 192)  0           batch_normalization_145[0][0]    
__________________________________________________________________________________________________
activation_148 (Activation)     (None, 17, 17, 192)  0           batch_normalization_148[0][0]    
__________________________________________________________________________________________________
activation_153 (Activation)     (None, 17, 17, 192)  0           batch_normalization_153[0][0]    
__________________________________________________________________________________________________
activation_154 (Activation)     (None, 17, 17, 192)  0           batch_normalization_154[0][0]    
__________________________________________________________________________________________________
mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_145[0][0]             
                                                                 activation_148[0][0]             
                                                                 activation_153[0][0]             
                                                                 activation_154[0][0]             
__________________________________________________________________________________________________
conv2d_159 (Conv2D)             (None, 17, 17, 192)  147456      mixed6[0][0]                     
__________________________________________________________________________________________________
batch_normalization_159 (BatchN (None, 17, 17, 192)  576         conv2d_159[0][0]                 
__________________________________________________________________________________________________
activation_159 (Activation)     (None, 17, 17, 192)  0           batch_normalization_159[0][0]    
__________________________________________________________________________________________________
conv2d_160 (Conv2D)             (None, 17, 17, 192)  258048      activation_159[0][0]             
__________________________________________________________________________________________________
batch_normalization_160 (BatchN (None, 17, 17, 192)  576         conv2d_160[0][0]                 
__________________________________________________________________________________________________
activation_160 (Activation)     (None, 17, 17, 192)  0           batch_normalization_160[0][0]    
__________________________________________________________________________________________________
conv2d_156 (Conv2D)             (None, 17, 17, 192)  147456      mixed6[0][0]                     
__________________________________________________________________________________________________
conv2d_161 (Conv2D)             (None, 17, 17, 192)  258048      activation_160[0][0]             
__________________________________________________________________________________________________
batch_normalization_156 (BatchN (None, 17, 17, 192)  576         conv2d_156[0][0]                 
__________________________________________________________________________________________________
batch_normalization_161 (BatchN (None, 17, 17, 192)  576         conv2d_161[0][0]                 
__________________________________________________________________________________________________
activation_156 (Activation)     (None, 17, 17, 192)  0           batch_normalization_156[0][0]    
__________________________________________________________________________________________________
activation_161 (Activation)     (None, 17, 17, 192)  0           batch_normalization_161[0][0]    
__________________________________________________________________________________________________
conv2d_157 (Conv2D)             (None, 17, 17, 192)  258048      activation_156[0][0]             
__________________________________________________________________________________________________
conv2d_162 (Conv2D)             (None, 17, 17, 192)  258048      activation_161[0][0]             
__________________________________________________________________________________________________
batch_normalization_157 (BatchN (None, 17, 17, 192)  576         conv2d_157[0][0]                 
__________________________________________________________________________________________________
batch_normalization_162 (BatchN (None, 17, 17, 192)  576         conv2d_162[0][0]                 
__________________________________________________________________________________________________
activation_157 (Activation)     (None, 17, 17, 192)  0           batch_normalization_157[0][0]    
__________________________________________________________________________________________________
activation_162 (Activation)     (None, 17, 17, 192)  0           batch_normalization_162[0][0]    
__________________________________________________________________________________________________
average_pooling2d_16 (AveragePo (None, 17, 17, 768)  0           mixed6[0][0]                     
__________________________________________________________________________________________________
conv2d_155 (Conv2D)             (None, 17, 17, 192)  147456      mixed6[0][0]                     
__________________________________________________________________________________________________
conv2d_158 (Conv2D)             (None, 17, 17, 192)  258048      activation_157[0][0]             
__________________________________________________________________________________________________
conv2d_163 (Conv2D)             (None, 17, 17, 192)  258048      activation_162[0][0]             
__________________________________________________________________________________________________
conv2d_164 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_16[0][0]       
__________________________________________________________________________________________________
batch_normalization_155 (BatchN (None, 17, 17, 192)  576         conv2d_155[0][0]                 
__________________________________________________________________________________________________
batch_normalization_158 (BatchN (None, 17, 17, 192)  576         conv2d_158[0][0]                 
__________________________________________________________________________________________________
batch_normalization_163 (BatchN (None, 17, 17, 192)  576         conv2d_163[0][0]                 
__________________________________________________________________________________________________
batch_normalization_164 (BatchN (None, 17, 17, 192)  576         conv2d_164[0][0]                 
__________________________________________________________________________________________________
activation_155 (Activation)     (None, 17, 17, 192)  0           batch_normalization_155[0][0]    
__________________________________________________________________________________________________
activation_158 (Activation)     (None, 17, 17, 192)  0           batch_normalization_158[0][0]    
__________________________________________________________________________________________________
activation_163 (Activation)     (None, 17, 17, 192)  0           batch_normalization_163[0][0]    
__________________________________________________________________________________________________
activation_164 (Activation)     (None, 17, 17, 192)  0           batch_normalization_164[0][0]    
__________________________________________________________________________________________________
mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_155[0][0]             
                                                                 activation_158[0][0]             
                                                                 activation_163[0][0]             
                                                                 activation_164[0][0]             
__________________________________________________________________________________________________
conv2d_167 (Conv2D)             (None, 17, 17, 192)  147456      mixed7[0][0]                     
__________________________________________________________________________________________________
batch_normalization_167 (BatchN (None, 17, 17, 192)  576         conv2d_167[0][0]                 
__________________________________________________________________________________________________
activation_167 (Activation)     (None, 17, 17, 192)  0           batch_normalization_167[0][0]    
__________________________________________________________________________________________________
conv2d_168 (Conv2D)             (None, 17, 17, 192)  258048      activation_167[0][0]             
__________________________________________________________________________________________________
batch_normalization_168 (BatchN (None, 17, 17, 192)  576         conv2d_168[0][0]                 
__________________________________________________________________________________________________
activation_168 (Activation)     (None, 17, 17, 192)  0           batch_normalization_168[0][0]    
__________________________________________________________________________________________________
conv2d_165 (Conv2D)             (None, 17, 17, 192)  147456      mixed7[0][0]                     
__________________________________________________________________________________________________
conv2d_169 (Conv2D)             (None, 17, 17, 192)  258048      activation_168[0][0]             
__________________________________________________________________________________________________
batch_normalization_165 (BatchN (None, 17, 17, 192)  576         conv2d_165[0][0]                 
__________________________________________________________________________________________________
batch_normalization_169 (BatchN (None, 17, 17, 192)  576         conv2d_169[0][0]                 
__________________________________________________________________________________________________
activation_165 (Activation)     (None, 17, 17, 192)  0           batch_normalization_165[0][0]    
__________________________________________________________________________________________________
activation_169 (Activation)     (None, 17, 17, 192)  0           batch_normalization_169[0][0]    
__________________________________________________________________________________________________
conv2d_166 (Conv2D)             (None, 8, 8, 320)    552960      activation_165[0][0]             
__________________________________________________________________________________________________
conv2d_170 (Conv2D)             (None, 8, 8, 192)    331776      activation_169[0][0]             
__________________________________________________________________________________________________
batch_normalization_166 (BatchN (None, 8, 8, 320)    960         conv2d_166[0][0]                 
__________________________________________________________________________________________________
batch_normalization_170 (BatchN (None, 8, 8, 192)    576         conv2d_170[0][0]                 
__________________________________________________________________________________________________
activation_166 (Activation)     (None, 8, 8, 320)    0           batch_normalization_166[0][0]    
__________________________________________________________________________________________________
activation_170 (Activation)     (None, 8, 8, 192)    0           batch_normalization_170[0][0]    
__________________________________________________________________________________________________
max_pooling2d_8 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]                     
__________________________________________________________________________________________________
mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_166[0][0]             
                                                                 activation_170[0][0]             
                                                                 max_pooling2d_8[0][0]            
__________________________________________________________________________________________________
conv2d_175 (Conv2D)             (None, 8, 8, 448)    573440      mixed8[0][0]                     
__________________________________________________________________________________________________
batch_normalization_175 (BatchN (None, 8, 8, 448)    1344        conv2d_175[0][0]                 
__________________________________________________________________________________________________
activation_175 (Activation)     (None, 8, 8, 448)    0           batch_normalization_175[0][0]    
__________________________________________________________________________________________________
conv2d_172 (Conv2D)             (None, 8, 8, 384)    491520      mixed8[0][0]                     
__________________________________________________________________________________________________
conv2d_176 (Conv2D)             (None, 8, 8, 384)    1548288     activation_175[0][0]             
__________________________________________________________________________________________________
batch_normalization_172 (BatchN (None, 8, 8, 384)    1152        conv2d_172[0][0]                 
__________________________________________________________________________________________________
batch_normalization_176 (BatchN (None, 8, 8, 384)    1152        conv2d_176[0][0]                 
__________________________________________________________________________________________________
activation_172 (Activation)     (None, 8, 8, 384)    0           batch_normalization_172[0][0]    
__________________________________________________________________________________________________
activation_176 (Activation)     (None, 8, 8, 384)    0           batch_normalization_176[0][0]    
__________________________________________________________________________________________________
conv2d_173 (Conv2D)             (None, 8, 8, 384)    442368      activation_172[0][0]             
__________________________________________________________________________________________________
conv2d_174 (Conv2D)             (None, 8, 8, 384)    442368      activation_172[0][0]             
__________________________________________________________________________________________________
conv2d_177 (Conv2D)             (None, 8, 8, 384)    442368      activation_176[0][0]             
__________________________________________________________________________________________________
conv2d_178 (Conv2D)             (None, 8, 8, 384)    442368      activation_176[0][0]             
__________________________________________________________________________________________________
average_pooling2d_17 (AveragePo (None, 8, 8, 1280)   0           mixed8[0][0]                     
__________________________________________________________________________________________________
conv2d_171 (Conv2D)             (None, 8, 8, 320)    409600      mixed8[0][0]                     
__________________________________________________________________________________________________
batch_normalization_173 (BatchN (None, 8, 8, 384)    1152        conv2d_173[0][0]                 
__________________________________________________________________________________________________
batch_normalization_174 (BatchN (None, 8, 8, 384)    1152        conv2d_174[0][0]                 
__________________________________________________________________________________________________
batch_normalization_177 (BatchN (None, 8, 8, 384)    1152        conv2d_177[0][0]                 
__________________________________________________________________________________________________
batch_normalization_178 (BatchN (None, 8, 8, 384)    1152        conv2d_178[0][0]                 
__________________________________________________________________________________________________
conv2d_179 (Conv2D)             (None, 8, 8, 192)    245760      average_pooling2d_17[0][0]       
__________________________________________________________________________________________________
batch_normalization_171 (BatchN (None, 8, 8, 320)    960         conv2d_171[0][0]                 
__________________________________________________________________________________________________
activation_173 (Activation)     (None, 8, 8, 384)    0           batch_normalization_173[0][0]    
__________________________________________________________________________________________________
activation_174 (Activation)     (None, 8, 8, 384)    0           batch_normalization_174[0][0]    
__________________________________________________________________________________________________
activation_177 (Activation)     (None, 8, 8, 384)    0           batch_normalization_177[0][0]    
__________________________________________________________________________________________________
activation_178 (Activation)     (None, 8, 8, 384)    0           batch_normalization_178[0][0]    
__________________________________________________________________________________________________
batch_normalization_179 (BatchN (None, 8, 8, 192)    576         conv2d_179[0][0]                 
__________________________________________________________________________________________________
activation_171 (Activation)     (None, 8, 8, 320)    0           batch_normalization_171[0][0]    
__________________________________________________________________________________________________
mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_173[0][0]             
                                                                 activation_174[0][0]             
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 8, 8, 768)    0           activation_177[0][0]             
                                                                 activation_178[0][0]             
__________________________________________________________________________________________________
activation_179 (Activation)     (None, 8, 8, 192)    0           batch_normalization_179[0][0]    
__________________________________________________________________________________________________
mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_171[0][0]             
                                                                 mixed9_0[0][0]                   
                                                                 concatenate_3[0][0]              
                                                                 activation_179[0][0]             
__________________________________________________________________________________________________
conv2d_184 (Conv2D)             (None, 8, 8, 448)    917504      mixed9[0][0]                     
__________________________________________________________________________________________________
batch_normalization_184 (BatchN (None, 8, 8, 448)    1344        conv2d_184[0][0]                 
__________________________________________________________________________________________________
activation_184 (Activation)     (None, 8, 8, 448)    0           batch_normalization_184[0][0]    
__________________________________________________________________________________________________
conv2d_181 (Conv2D)             (None, 8, 8, 384)    786432      mixed9[0][0]                     
__________________________________________________________________________________________________
conv2d_185 (Conv2D)             (None, 8, 8, 384)    1548288     activation_184[0][0]             
__________________________________________________________________________________________________
batch_normalization_181 (BatchN (None, 8, 8, 384)    1152        conv2d_181[0][0]                 
__________________________________________________________________________________________________
batch_normalization_185 (BatchN (None, 8, 8, 384)    1152        conv2d_185[0][0]                 
__________________________________________________________________________________________________
activation_181 (Activation)     (None, 8, 8, 384)    0           batch_normalization_181[0][0]    
__________________________________________________________________________________________________
activation_185 (Activation)     (None, 8, 8, 384)    0           batch_normalization_185[0][0]    
__________________________________________________________________________________________________
conv2d_182 (Conv2D)             (None, 8, 8, 384)    442368      activation_181[0][0]             
__________________________________________________________________________________________________
conv2d_183 (Conv2D)             (None, 8, 8, 384)    442368      activation_181[0][0]             
__________________________________________________________________________________________________
conv2d_186 (Conv2D)             (None, 8, 8, 384)    442368      activation_185[0][0]             
__________________________________________________________________________________________________
conv2d_187 (Conv2D)             (None, 8, 8, 384)    442368      activation_185[0][0]             
__________________________________________________________________________________________________
average_pooling2d_18 (AveragePo (None, 8, 8, 2048)   0           mixed9[0][0]                     
__________________________________________________________________________________________________
conv2d_180 (Conv2D)             (None, 8, 8, 320)    655360      mixed9[0][0]                     
__________________________________________________________________________________________________
batch_normalization_182 (BatchN (None, 8, 8, 384)    1152        conv2d_182[0][0]                 
__________________________________________________________________________________________________
batch_normalization_183 (BatchN (None, 8, 8, 384)    1152        conv2d_183[0][0]                 
__________________________________________________________________________________________________
batch_normalization_186 (BatchN (None, 8, 8, 384)    1152        conv2d_186[0][0]                 
__________________________________________________________________________________________________
batch_normalization_187 (BatchN (None, 8, 8, 384)    1152        conv2d_187[0][0]                 
__________________________________________________________________________________________________
conv2d_188 (Conv2D)             (None, 8, 8, 192)    393216      average_pooling2d_18[0][0]       
__________________________________________________________________________________________________
batch_normalization_180 (BatchN (None, 8, 8, 320)    960         conv2d_180[0][0]                 
__________________________________________________________________________________________________
activation_182 (Activation)     (None, 8, 8, 384)    0           batch_normalization_182[0][0]    
__________________________________________________________________________________________________
activation_183 (Activation)     (None, 8, 8, 384)    0           batch_normalization_183[0][0]    
__________________________________________________________________________________________________
activation_186 (Activation)     (None, 8, 8, 384)    0           batch_normalization_186[0][0]    
__________________________________________________________________________________________________
activation_187 (Activation)     (None, 8, 8, 384)    0           batch_normalization_187[0][0]    
__________________________________________________________________________________________________
batch_normalization_188 (BatchN (None, 8, 8, 192)    576         conv2d_188[0][0]                 
__________________________________________________________________________________________________
activation_180 (Activation)     (None, 8, 8, 320)    0           batch_normalization_180[0][0]    
__________________________________________________________________________________________________
mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_182[0][0]             
                                                                 activation_183[0][0]             
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 8, 8, 768)    0           activation_186[0][0]             
                                                                 activation_187[0][0]             
__________________________________________________________________________________________________
activation_188 (Activation)     (None, 8, 8, 192)    0           batch_normalization_188[0][0]    
__________________________________________________________________________________________________
mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_180[0][0]             
                                                                 mixed9_1[0][0]                   
                                                                 concatenate_4[0][0]              
                                                                 activation_188[0][0]             
__________________________________________________________________________________________________
global_average_pooling2d_2 (Glo (None, 2048)         0           mixed10[0][0]                    
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1024)         2098176     global_average_pooling2d_2[0][0] 
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 10)           10250       dense_3[0][0]                    
==================================================================================================
Total params: 23,911,210
Trainable params: 2,108,426
Non-trainable params: 21,802,784
__________________________________________________________________________________________________
Start Training / Training Callbacks
Callback and Training

To start our training we're going to utilize the Keras Fit Generator method. This will allow us to pass in the ImageDataGenerator/flow_from_directory structure from above directly to the method. Here is where we can also set our epochs, validation set, and callbacks.

Setting an epoch count can really depend on the dataset that you're using and how long you're willing to wait for the training to complete. During out testing phase we ended up trying 100 epoch's and utilizing Callbacks to make sure that we're not overfitting.

It's useful to have a validation set that you can try to maximize toward when doing your training. This way you have an idea of how well things are going on a small subset of data that isn't the test set. If you use the test set for this purpose it will be harder to tell if you're generalizing well or if you're only fitting to the test set specifically and won't do well in a real world test. Earlier in the notebook we insantiated a validation set generator which we can pass in here by setting the generator to the validation_data paramter.

We need to specify both the validation and training number of steps per epoch. We can use variables defined in our generators so that we're not hard coding numbers directly in these fields. This is done by taking our sample size and dividing by our batch size then utilizing the math.ceil function so we properly account for the final smaller batch in the dataset.

Lastly, we define 4 different callbacks that we're going to use during our training. Callbacks are called after an epoch of training is complete and then will perform whatever action is necessary.

Callbacks
ModelCheckpoint: Used to save our checkpoint based on a certain set of criteria. In this case we're monitor our loss and looking for the min value. After an epoch it will check to see if the loss is lower than the previous epoch of training and if so then to save the model to disk. We're also indicating that we want to save only the best model so the previous model will be overwritten. This will minimize the amount of disk space taken up by models.

TensorBoard: Utilizing this checkpoint will write out the tf.events file during training so we can view them in TensorBoard. We need to indicate a location for the logs to be saved and the update frequence for which we want the data. In our case we're asking for batch frequence data so we need to also pass in the batch size to the function. We can then start Tensorboard and point the --logdir parameter at the directory where the logs are saved to see how our training is going.

EarlyStopping: This is used to make sure that we're not overtraining and overutilizing our compute. It will monitor a metric after every epoch and see how much it has changed since the previous epoch. If it hasn't changed significantly over the last n number of epochs it will end the training prematurely since there isn't much gain from going further. In this case we're monitoring loss and asking for it to minimize the value, we also give a patience value of 5 which means the value has to not change significantly for 5 epochs before the training will be stopped.

CSVLogger: Another logging technique that can be easier to read by directly inspecting the log file. This can be useful if you're not interested in using TensorBoard or just want to use a simpler logging output. It will write out the metrics we're keeping track of during training and append them to the CSV file giving during the instanitation of the callback.

Activity
In the cell below, update epochs to 5 , and then click Run.

samples
import math
top_layers_file_path="top_layers.iv3.hdf5"
​
checkpoint = ModelCheckpoint(top_layers_file_path, monitor='loss', verbose=1, save_best_only=True, mode='min')
tb = TensorBoard(log_dir='./logs', batch_size=val_flow.batch_size, write_graph=True, update_freq='batch')
early = EarlyStopping(monitor="loss", mode="min", patience=5)
csv_logger = CSVLogger('./logs/iv3-log.csv', append=True)
​
history = model.fit_generator(train_flow, 
                              epochs=1, 
                              verbose=1,
                              validation_data=val_flow,
                              validation_steps=math.ceil(val_flow.samples/val_flow.batch_size),
                              steps_per_epoch=math.ceil(train_flow.samples/train_flow.batch_size),
                              callbacks=[checkpoint, early, tb, csv_logger])
WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tf_training/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
Epoch 1/1
80/80 [==============================] - 172s 2s/step - loss: 1.7072 - acc: 0.4088 - top_k_categorical_accuracy: 0.8840 - val_loss: 1.6806 - val_acc: 0.3294 - val_top_k_categorical_accuracy: 0.9215

Epoch 00001: loss improved from inf to 1.70991, saving model to top_layers.iv3.hdf5
Evaluate Model
Evaluate Model

To evaluate our training results we can use the evaluation generator. This will integrate well with our data generator from the previous step that points to our testing set. We will again have to specify the amount of steps used by calculating with samples and batch size and indicated earlier.

We're also going to receive three different results from our evaluate generator: loss, accuracy and top k accuracy. These are given since when we compiled our model we indicated that we're evaluting for categorical crossentropy for loss and our metrics were accuracy and top_k_categorical_accuracy which will evaluate if our result was in the top 5 confidence. If we added or removed a metric during compiliation it would be reflected here in our evaluation generator and we would get more or less values in return from this function.

Activity
Click the cell below and then click Run.

model.load_weights(top_layers_file_path)
loss, acc, top_5 = model.evaluate_generator(
    test_flow,
    verbose = True,
    steps=math.ceil(test_flow.samples/test_flow.batch_size))
print("Loss: ", loss)
print("Acc: ", acc)
print("Top 5: ", top_5)
19/19 [==============================] - 37s 2s/step
Loss:  1.6822193457314873
Acc:  0.32690695675100884
Top 5:  0.9262363784271234
Write Labels File
In Deep learning, when we're training our network we're using a numerical value for the actual class that is predicated at the end of each batch through the network. The network itself doesn't care what the actual string class name is, only that it's optimizing for one of the n classes you have in your dataset.

So when we move forward and use our network we need to indicate what numerical value the network was using to represent the correct class name. We can do this by iterating over any of the data generator class_indicies values and use a list comprehension to extract the values. We're going to write these values out in order to a text file to represent the numerical value mapping to class name for future use.

Activity
Click the cell below and then click Run.

label = [k for k,v in train_flow.class_indices.items()]
with open('iv3-labels.txt', 'w+') as file:
    file.write("\n".join(label))
Test Model with Sample image
Test Model

Now that we're done training we want to see the results for our model on a test image. We're going to load a random image chosen from our test set and run it through the model.

We can use Keras to load the image and resize it to the given size required for the network. We then need to convert the Image object to an array and add an extra axis to the array so it's in the format (n, h, w, c). Then it will be run through the preprocessing step required for all inputs into the network and passed into the model.predict run for the results.

After running the image through the model for a prediction we need to map the indicies to the correct class names and we can also take a look at the confidence values returned by the softmax.

To do this we'll start by sorting the prediction results by index. Then we can get the top X number of values from the end of the array and then we'll reorder the array in the reverse order since the highest confidence is the last value in the array. We can then use this array to read the confidence and label of the top X results.

Activity
Click the cell below and then click Run.

from keras.preprocessing import image
import numpy as np
import glob
import random
​
file_list = glob.glob("test/*/*")
img_path = random.choice(file_list)
img_cat = os.path.split(os.path.dirname(img_path))[1]
print("Image Category: ", img_cat)
img = image.load_img(img_path, target_size=(299, 299))
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x = preprocess_input(x)
​
preds = model.predict(x)
print("Raw Predictions: ", preds)
​
top_x = 3
top_args = preds[0].argsort()[-top_x:][::-1]
preds_label = [label[p] for p in top_args]
print("\nTop " + str(top_x) + " confidence: " + " ".join(map(str, sorted(preds[0])[-top_x:][::-1])))
print("Top " + str(top_x) + " labels: " + " ".join(map(str, preds_label)))
Image Category:  honda_accord_1997
Raw Predictions:  [[0.01265356 0.0134322  0.05430479 0.05009959 0.00093155 0.73836106
  0.09082673 0.01406337 0.02081155 0.00451557]]

Top 3 confidence: 0.73836106 0.090826735 0.054304793
Top 3 labels: honda_accord_1997 honda_civic_1998 dodge_ram_2001
Transform Keras Model to Tensorflow Frozen Graph
Freeze Graph

Keras utilizes the h5 or hdf5 file format when saving its model. If we want to use our model outside of Keras, in OpenVINO, we need a frozen pb file to pass in when using a Tensorflow model. We can do that directly from Keras by utilizing the below functions.

First we need to make sure that you set the learning phase to 0 or you might end up not correctly getting the output node from the session. Then we grab the session and output names and pass them to graph_util.convert_variables_to_constants.

"If you have a trained graph containing Variable ops, it can be convenient to convert them all to Const ops holding the same values. This makes it possible to describe the network fully with a single GraphDef file, and allows the removal of a lot of ops related to loading and saving the variables." https://www.tensorflow.org/api_docs/python/tf/graph_util/convert_variables_to_constants

and then we pass that constant graph to graph_io.write_graph which writes the graph proto to a file.

Activity
Click the cell below and then click Run.

from tensorflow.python.framework import graph_util
from tensorflow.python.framework import graph_io
​
input_model_path = top_layers_file_path
output_model_name = "top_layers.iv3.pb"
output_model_dir = "tf_model"
​
K.set_learning_phase(0)
sess = K.get_session()
​
test_model = models.load_model(input_model_path)
orig_output_node_names = [node.op.name for node in test_model.outputs]
​
constant_graph = graph_util.convert_variables_to_constants(
    sess,
    sess.graph.as_graph_def(),
    orig_output_node_names)
graph_io.write_graph(
    constant_graph,
    output_model_dir,
    output_model_name,
    as_text=False)
WARNING:tensorflow:From <ipython-input-12-258782380f86>:17: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.compat.v1.graph_util.convert_variables_to_constants
WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tf_training/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.compat.v1.graph_util.extract_sub_graph
INFO:tensorflow:Froze 380 variables.
INFO:tensorflow:Converted 380 variables to const ops.
'tf_model/top_layers.iv3.pb'
%%html
<h3>Tensorflow Training - Quiz</h3>
<p><a target="_blank" href="https://intel.az1.qualtrics.com/jfe/form/SV_9EIVi2JXNF1ViiV">Take the quiz</a> or using the form below (Adblocker may remove form)<p>
<br>
<iframe style="border: 1px dashed #0071c5;" src="https://intel.az1.qualtrics.com/jfe/form/SV_9EIVi2JXNF1ViiV" title="DC2E Training Cert Quiz" width="950" height="800"></iframe>
Tensorflow Training - Quiz
Take the quiz or using the form below (Adblocker may remove form)




Summary - Initial Training Done
You've learned about the following:

Create DataGenerator for your dataset
Learn about CPU Optimization for Tensorflow
Understand Hyperparameter Selection
Compile your model
Learn about callbacks
Start your training
Evaluate Your Model
Test Your Model on a sample image
Freeze your graph
We've now completed the first round of training! You can now utilize the frozen graph from the end of this section to run inference with through OpenVINO.

Optional - Additional Training for the Entire Network
Fine Tuning the Entire Network
We previously fine tuned only the top layer of the network. Now we're going to allow for all of the layers in the network to be trained but we're going to use a lower learning rate. This will let the network narrow in and tune the remaining weights we didn't tune from the ImageNet checkpoint.

We'll start by unfreezing the top two inception layers in our model and then compiling the model again. The remaining pieces of the code will be almost identical to the above except that we're making sure to change file path names that indicate we're utilizing the top two inception nodes in this training.

model = load_model(top_layers_file_path)
​
# we chose to train the top 2 inception blocks, i.e. we will freeze
# the first 249 layers and unfreeze the rest:
for layer in model.layers[:249]:
    layer.trainable = False
for layer in model.layers[249:]:
    layer.trainable = True
​
# we need to recompile the model for these modifications to take effect
# we use Adam with a low learning rate
model.compile(optimizer=optimizers.Adam(lr=0.0001), metrics=['accuracy', 'top_k_categorical_accuracy'], loss='categorical_crossentropy')
model.summary()
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            (None, 299, 299, 3)  0                                            
__________________________________________________________________________________________________
conv2d_95 (Conv2D)              (None, 149, 149, 32) 864         input_2[0][0]                    
__________________________________________________________________________________________________
batch_normalization_95 (BatchNo (None, 149, 149, 32) 96          conv2d_95[0][0]                  
__________________________________________________________________________________________________
activation_95 (Activation)      (None, 149, 149, 32) 0           batch_normalization_95[0][0]     
__________________________________________________________________________________________________
conv2d_96 (Conv2D)              (None, 147, 147, 32) 9216        activation_95[0][0]              
__________________________________________________________________________________________________
batch_normalization_96 (BatchNo (None, 147, 147, 32) 96          conv2d_96[0][0]                  
__________________________________________________________________________________________________
activation_96 (Activation)      (None, 147, 147, 32) 0           batch_normalization_96[0][0]     
__________________________________________________________________________________________________
conv2d_97 (Conv2D)              (None, 147, 147, 64) 18432       activation_96[0][0]              
__________________________________________________________________________________________________
batch_normalization_97 (BatchNo (None, 147, 147, 64) 192         conv2d_97[0][0]                  
__________________________________________________________________________________________________
activation_97 (Activation)      (None, 147, 147, 64) 0           batch_normalization_97[0][0]     
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_97[0][0]              
__________________________________________________________________________________________________
conv2d_98 (Conv2D)              (None, 73, 73, 80)   5120        max_pooling2d_5[0][0]            
__________________________________________________________________________________________________
batch_normalization_98 (BatchNo (None, 73, 73, 80)   240         conv2d_98[0][0]                  
__________________________________________________________________________________________________
activation_98 (Activation)      (None, 73, 73, 80)   0           batch_normalization_98[0][0]     
__________________________________________________________________________________________________
conv2d_99 (Conv2D)              (None, 71, 71, 192)  138240      activation_98[0][0]              
__________________________________________________________________________________________________
batch_normalization_99 (BatchNo (None, 71, 71, 192)  576         conv2d_99[0][0]                  
__________________________________________________________________________________________________
activation_99 (Activation)      (None, 71, 71, 192)  0           batch_normalization_99[0][0]     
__________________________________________________________________________________________________
max_pooling2d_6 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_99[0][0]              
__________________________________________________________________________________________________
conv2d_103 (Conv2D)             (None, 35, 35, 64)   12288       max_pooling2d_6[0][0]            
__________________________________________________________________________________________________
batch_normalization_103 (BatchN (None, 35, 35, 64)   192         conv2d_103[0][0]                 
__________________________________________________________________________________________________
activation_103 (Activation)     (None, 35, 35, 64)   0           batch_normalization_103[0][0]    
__________________________________________________________________________________________________
conv2d_101 (Conv2D)             (None, 35, 35, 48)   9216        max_pooling2d_6[0][0]            
__________________________________________________________________________________________________
conv2d_104 (Conv2D)             (None, 35, 35, 96)   55296       activation_103[0][0]             
__________________________________________________________________________________________________
batch_normalization_101 (BatchN (None, 35, 35, 48)   144         conv2d_101[0][0]                 
__________________________________________________________________________________________________
batch_normalization_104 (BatchN (None, 35, 35, 96)   288         conv2d_104[0][0]                 
__________________________________________________________________________________________________
activation_101 (Activation)     (None, 35, 35, 48)   0           batch_normalization_101[0][0]    
__________________________________________________________________________________________________
activation_104 (Activation)     (None, 35, 35, 96)   0           batch_normalization_104[0][0]    
__________________________________________________________________________________________________
average_pooling2d_10 (AveragePo (None, 35, 35, 192)  0           max_pooling2d_6[0][0]            
__________________________________________________________________________________________________
conv2d_100 (Conv2D)             (None, 35, 35, 64)   12288       max_pooling2d_6[0][0]            
__________________________________________________________________________________________________
conv2d_102 (Conv2D)             (None, 35, 35, 64)   76800       activation_101[0][0]             
__________________________________________________________________________________________________
conv2d_105 (Conv2D)             (None, 35, 35, 96)   82944       activation_104[0][0]             
__________________________________________________________________________________________________
conv2d_106 (Conv2D)             (None, 35, 35, 32)   6144        average_pooling2d_10[0][0]       
__________________________________________________________________________________________________
batch_normalization_100 (BatchN (None, 35, 35, 64)   192         conv2d_100[0][0]                 
__________________________________________________________________________________________________
batch_normalization_102 (BatchN (None, 35, 35, 64)   192         conv2d_102[0][0]                 
__________________________________________________________________________________________________
batch_normalization_105 (BatchN (None, 35, 35, 96)   288         conv2d_105[0][0]                 
__________________________________________________________________________________________________
batch_normalization_106 (BatchN (None, 35, 35, 32)   96          conv2d_106[0][0]                 
__________________________________________________________________________________________________
activation_100 (Activation)     (None, 35, 35, 64)   0           batch_normalization_100[0][0]    
__________________________________________________________________________________________________
activation_102 (Activation)     (None, 35, 35, 64)   0           batch_normalization_102[0][0]    
__________________________________________________________________________________________________
activation_105 (Activation)     (None, 35, 35, 96)   0           batch_normalization_105[0][0]    
__________________________________________________________________________________________________
activation_106 (Activation)     (None, 35, 35, 32)   0           batch_normalization_106[0][0]    
__________________________________________________________________________________________________
mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_100[0][0]             
                                                                 activation_102[0][0]             
                                                                 activation_105[0][0]             
                                                                 activation_106[0][0]             
__________________________________________________________________________________________________
conv2d_110 (Conv2D)             (None, 35, 35, 64)   16384       mixed0[0][0]                     
__________________________________________________________________________________________________
batch_normalization_110 (BatchN (None, 35, 35, 64)   192         conv2d_110[0][0]                 
__________________________________________________________________________________________________
activation_110 (Activation)     (None, 35, 35, 64)   0           batch_normalization_110[0][0]    
__________________________________________________________________________________________________
conv2d_108 (Conv2D)             (None, 35, 35, 48)   12288       mixed0[0][0]                     
__________________________________________________________________________________________________
conv2d_111 (Conv2D)             (None, 35, 35, 96)   55296       activation_110[0][0]             
__________________________________________________________________________________________________
batch_normalization_108 (BatchN (None, 35, 35, 48)   144         conv2d_108[0][0]                 
__________________________________________________________________________________________________
batch_normalization_111 (BatchN (None, 35, 35, 96)   288         conv2d_111[0][0]                 
__________________________________________________________________________________________________
activation_108 (Activation)     (None, 35, 35, 48)   0           batch_normalization_108[0][0]    
__________________________________________________________________________________________________
activation_111 (Activation)     (None, 35, 35, 96)   0           batch_normalization_111[0][0]    
__________________________________________________________________________________________________
average_pooling2d_11 (AveragePo (None, 35, 35, 256)  0           mixed0[0][0]                     
__________________________________________________________________________________________________
conv2d_107 (Conv2D)             (None, 35, 35, 64)   16384       mixed0[0][0]                     
__________________________________________________________________________________________________
conv2d_109 (Conv2D)             (None, 35, 35, 64)   76800       activation_108[0][0]             
__________________________________________________________________________________________________
conv2d_112 (Conv2D)             (None, 35, 35, 96)   82944       activation_111[0][0]             
__________________________________________________________________________________________________
conv2d_113 (Conv2D)             (None, 35, 35, 64)   16384       average_pooling2d_11[0][0]       
__________________________________________________________________________________________________
batch_normalization_107 (BatchN (None, 35, 35, 64)   192         conv2d_107[0][0]                 
__________________________________________________________________________________________________
batch_normalization_109 (BatchN (None, 35, 35, 64)   192         conv2d_109[0][0]                 
__________________________________________________________________________________________________
batch_normalization_112 (BatchN (None, 35, 35, 96)   288         conv2d_112[0][0]                 
__________________________________________________________________________________________________
batch_normalization_113 (BatchN (None, 35, 35, 64)   192         conv2d_113[0][0]                 
__________________________________________________________________________________________________
activation_107 (Activation)     (None, 35, 35, 64)   0           batch_normalization_107[0][0]    
__________________________________________________________________________________________________
activation_109 (Activation)     (None, 35, 35, 64)   0           batch_normalization_109[0][0]    
__________________________________________________________________________________________________
activation_112 (Activation)     (None, 35, 35, 96)   0           batch_normalization_112[0][0]    
__________________________________________________________________________________________________
activation_113 (Activation)     (None, 35, 35, 64)   0           batch_normalization_113[0][0]    
__________________________________________________________________________________________________
mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_107[0][0]             
                                                                 activation_109[0][0]             
                                                                 activation_112[0][0]             
                                                                 activation_113[0][0]             
__________________________________________________________________________________________________
conv2d_117 (Conv2D)             (None, 35, 35, 64)   18432       mixed1[0][0]                     
__________________________________________________________________________________________________
batch_normalization_117 (BatchN (None, 35, 35, 64)   192         conv2d_117[0][0]                 
__________________________________________________________________________________________________
activation_117 (Activation)     (None, 35, 35, 64)   0           batch_normalization_117[0][0]    
__________________________________________________________________________________________________
conv2d_115 (Conv2D)             (None, 35, 35, 48)   13824       mixed1[0][0]                     
__________________________________________________________________________________________________
conv2d_118 (Conv2D)             (None, 35, 35, 96)   55296       activation_117[0][0]             
__________________________________________________________________________________________________
batch_normalization_115 (BatchN (None, 35, 35, 48)   144         conv2d_115[0][0]                 
__________________________________________________________________________________________________
batch_normalization_118 (BatchN (None, 35, 35, 96)   288         conv2d_118[0][0]                 
__________________________________________________________________________________________________
activation_115 (Activation)     (None, 35, 35, 48)   0           batch_normalization_115[0][0]    
__________________________________________________________________________________________________
activation_118 (Activation)     (None, 35, 35, 96)   0           batch_normalization_118[0][0]    
__________________________________________________________________________________________________
average_pooling2d_12 (AveragePo (None, 35, 35, 288)  0           mixed1[0][0]                     
__________________________________________________________________________________________________
conv2d_114 (Conv2D)             (None, 35, 35, 64)   18432       mixed1[0][0]                     
__________________________________________________________________________________________________
conv2d_116 (Conv2D)             (None, 35, 35, 64)   76800       activation_115[0][0]             
__________________________________________________________________________________________________
conv2d_119 (Conv2D)             (None, 35, 35, 96)   82944       activation_118[0][0]             
__________________________________________________________________________________________________
conv2d_120 (Conv2D)             (None, 35, 35, 64)   18432       average_pooling2d_12[0][0]       
__________________________________________________________________________________________________
batch_normalization_114 (BatchN (None, 35, 35, 64)   192         conv2d_114[0][0]                 
__________________________________________________________________________________________________
batch_normalization_116 (BatchN (None, 35, 35, 64)   192         conv2d_116[0][0]                 
__________________________________________________________________________________________________
batch_normalization_119 (BatchN (None, 35, 35, 96)   288         conv2d_119[0][0]                 
__________________________________________________________________________________________________
batch_normalization_120 (BatchN (None, 35, 35, 64)   192         conv2d_120[0][0]                 
__________________________________________________________________________________________________
activation_114 (Activation)     (None, 35, 35, 64)   0           batch_normalization_114[0][0]    
__________________________________________________________________________________________________
activation_116 (Activation)     (None, 35, 35, 64)   0           batch_normalization_116[0][0]    
__________________________________________________________________________________________________
activation_119 (Activation)     (None, 35, 35, 96)   0           batch_normalization_119[0][0]    
__________________________________________________________________________________________________
activation_120 (Activation)     (None, 35, 35, 64)   0           batch_normalization_120[0][0]    
__________________________________________________________________________________________________
mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_114[0][0]             
                                                                 activation_116[0][0]             
                                                                 activation_119[0][0]             
                                                                 activation_120[0][0]             
__________________________________________________________________________________________________
conv2d_122 (Conv2D)             (None, 35, 35, 64)   18432       mixed2[0][0]                     
__________________________________________________________________________________________________
batch_normalization_122 (BatchN (None, 35, 35, 64)   192         conv2d_122[0][0]                 
__________________________________________________________________________________________________
activation_122 (Activation)     (None, 35, 35, 64)   0           batch_normalization_122[0][0]    
__________________________________________________________________________________________________
conv2d_123 (Conv2D)             (None, 35, 35, 96)   55296       activation_122[0][0]             
__________________________________________________________________________________________________
batch_normalization_123 (BatchN (None, 35, 35, 96)   288         conv2d_123[0][0]                 
__________________________________________________________________________________________________
activation_123 (Activation)     (None, 35, 35, 96)   0           batch_normalization_123[0][0]    
__________________________________________________________________________________________________
conv2d_121 (Conv2D)             (None, 17, 17, 384)  995328      mixed2[0][0]                     
__________________________________________________________________________________________________
conv2d_124 (Conv2D)             (None, 17, 17, 96)   82944       activation_123[0][0]             
__________________________________________________________________________________________________
batch_normalization_121 (BatchN (None, 17, 17, 384)  1152        conv2d_121[0][0]                 
__________________________________________________________________________________________________
batch_normalization_124 (BatchN (None, 17, 17, 96)   288         conv2d_124[0][0]                 
__________________________________________________________________________________________________
activation_121 (Activation)     (None, 17, 17, 384)  0           batch_normalization_121[0][0]    
__________________________________________________________________________________________________
activation_124 (Activation)     (None, 17, 17, 96)   0           batch_normalization_124[0][0]    
__________________________________________________________________________________________________
max_pooling2d_7 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]                     
__________________________________________________________________________________________________
mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_121[0][0]             
                                                                 activation_124[0][0]             
                                                                 max_pooling2d_7[0][0]            
__________________________________________________________________________________________________
conv2d_129 (Conv2D)             (None, 17, 17, 128)  98304       mixed3[0][0]                     
__________________________________________________________________________________________________
batch_normalization_129 (BatchN (None, 17, 17, 128)  384         conv2d_129[0][0]                 
__________________________________________________________________________________________________
activation_129 (Activation)     (None, 17, 17, 128)  0           batch_normalization_129[0][0]    
__________________________________________________________________________________________________
conv2d_130 (Conv2D)             (None, 17, 17, 128)  114688      activation_129[0][0]             
__________________________________________________________________________________________________
batch_normalization_130 (BatchN (None, 17, 17, 128)  384         conv2d_130[0][0]                 
__________________________________________________________________________________________________
activation_130 (Activation)     (None, 17, 17, 128)  0           batch_normalization_130[0][0]    
__________________________________________________________________________________________________
conv2d_126 (Conv2D)             (None, 17, 17, 128)  98304       mixed3[0][0]                     
__________________________________________________________________________________________________
conv2d_131 (Conv2D)             (None, 17, 17, 128)  114688      activation_130[0][0]             
__________________________________________________________________________________________________
batch_normalization_126 (BatchN (None, 17, 17, 128)  384         conv2d_126[0][0]                 
__________________________________________________________________________________________________
batch_normalization_131 (BatchN (None, 17, 17, 128)  384         conv2d_131[0][0]                 
__________________________________________________________________________________________________
activation_126 (Activation)     (None, 17, 17, 128)  0           batch_normalization_126[0][0]    
__________________________________________________________________________________________________
activation_131 (Activation)     (None, 17, 17, 128)  0           batch_normalization_131[0][0]    
__________________________________________________________________________________________________
conv2d_127 (Conv2D)             (None, 17, 17, 128)  114688      activation_126[0][0]             
__________________________________________________________________________________________________
conv2d_132 (Conv2D)             (None, 17, 17, 128)  114688      activation_131[0][0]             
__________________________________________________________________________________________________
batch_normalization_127 (BatchN (None, 17, 17, 128)  384         conv2d_127[0][0]                 
__________________________________________________________________________________________________
batch_normalization_132 (BatchN (None, 17, 17, 128)  384         conv2d_132[0][0]                 
__________________________________________________________________________________________________
activation_127 (Activation)     (None, 17, 17, 128)  0           batch_normalization_127[0][0]    
__________________________________________________________________________________________________
activation_132 (Activation)     (None, 17, 17, 128)  0           batch_normalization_132[0][0]    
__________________________________________________________________________________________________
average_pooling2d_13 (AveragePo (None, 17, 17, 768)  0           mixed3[0][0]                     
__________________________________________________________________________________________________
conv2d_125 (Conv2D)             (None, 17, 17, 192)  147456      mixed3[0][0]                     
__________________________________________________________________________________________________
conv2d_128 (Conv2D)             (None, 17, 17, 192)  172032      activation_127[0][0]             
__________________________________________________________________________________________________
conv2d_133 (Conv2D)             (None, 17, 17, 192)  172032      activation_132[0][0]             
__________________________________________________________________________________________________
conv2d_134 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_13[0][0]       
__________________________________________________________________________________________________
batch_normalization_125 (BatchN (None, 17, 17, 192)  576         conv2d_125[0][0]                 
__________________________________________________________________________________________________
batch_normalization_128 (BatchN (None, 17, 17, 192)  576         conv2d_128[0][0]                 
__________________________________________________________________________________________________
batch_normalization_133 (BatchN (None, 17, 17, 192)  576         conv2d_133[0][0]                 
__________________________________________________________________________________________________
batch_normalization_134 (BatchN (None, 17, 17, 192)  576         conv2d_134[0][0]                 
__________________________________________________________________________________________________
activation_125 (Activation)     (None, 17, 17, 192)  0           batch_normalization_125[0][0]    
__________________________________________________________________________________________________
activation_128 (Activation)     (None, 17, 17, 192)  0           batch_normalization_128[0][0]    
__________________________________________________________________________________________________
activation_133 (Activation)     (None, 17, 17, 192)  0           batch_normalization_133[0][0]    
__________________________________________________________________________________________________
activation_134 (Activation)     (None, 17, 17, 192)  0           batch_normalization_134[0][0]    
__________________________________________________________________________________________________
mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_125[0][0]             
                                                                 activation_128[0][0]             
                                                                 activation_133[0][0]             
                                                                 activation_134[0][0]             
__________________________________________________________________________________________________
conv2d_139 (Conv2D)             (None, 17, 17, 160)  122880      mixed4[0][0]                     
__________________________________________________________________________________________________
batch_normalization_139 (BatchN (None, 17, 17, 160)  480         conv2d_139[0][0]                 
__________________________________________________________________________________________________
activation_139 (Activation)     (None, 17, 17, 160)  0           batch_normalization_139[0][0]    
__________________________________________________________________________________________________
conv2d_140 (Conv2D)             (None, 17, 17, 160)  179200      activation_139[0][0]             
__________________________________________________________________________________________________
batch_normalization_140 (BatchN (None, 17, 17, 160)  480         conv2d_140[0][0]                 
__________________________________________________________________________________________________
activation_140 (Activation)     (None, 17, 17, 160)  0           batch_normalization_140[0][0]    
__________________________________________________________________________________________________
conv2d_136 (Conv2D)             (None, 17, 17, 160)  122880      mixed4[0][0]                     
__________________________________________________________________________________________________
conv2d_141 (Conv2D)             (None, 17, 17, 160)  179200      activation_140[0][0]             
__________________________________________________________________________________________________
batch_normalization_136 (BatchN (None, 17, 17, 160)  480         conv2d_136[0][0]                 
__________________________________________________________________________________________________
batch_normalization_141 (BatchN (None, 17, 17, 160)  480         conv2d_141[0][0]                 
__________________________________________________________________________________________________
activation_136 (Activation)     (None, 17, 17, 160)  0           batch_normalization_136[0][0]    
__________________________________________________________________________________________________
activation_141 (Activation)     (None, 17, 17, 160)  0           batch_normalization_141[0][0]    
__________________________________________________________________________________________________
conv2d_137 (Conv2D)             (None, 17, 17, 160)  179200      activation_136[0][0]             
__________________________________________________________________________________________________
conv2d_142 (Conv2D)             (None, 17, 17, 160)  179200      activation_141[0][0]             
__________________________________________________________________________________________________
batch_normalization_137 (BatchN (None, 17, 17, 160)  480         conv2d_137[0][0]                 
__________________________________________________________________________________________________
batch_normalization_142 (BatchN (None, 17, 17, 160)  480         conv2d_142[0][0]                 
__________________________________________________________________________________________________
activation_137 (Activation)     (None, 17, 17, 160)  0           batch_normalization_137[0][0]    
__________________________________________________________________________________________________
activation_142 (Activation)     (None, 17, 17, 160)  0           batch_normalization_142[0][0]    
__________________________________________________________________________________________________
average_pooling2d_14 (AveragePo (None, 17, 17, 768)  0           mixed4[0][0]                     
__________________________________________________________________________________________________
conv2d_135 (Conv2D)             (None, 17, 17, 192)  147456      mixed4[0][0]                     
__________________________________________________________________________________________________
conv2d_138 (Conv2D)             (None, 17, 17, 192)  215040      activation_137[0][0]             
__________________________________________________________________________________________________
conv2d_143 (Conv2D)             (None, 17, 17, 192)  215040      activation_142[0][0]             
__________________________________________________________________________________________________
conv2d_144 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_14[0][0]       
__________________________________________________________________________________________________
batch_normalization_135 (BatchN (None, 17, 17, 192)  576         conv2d_135[0][0]                 
__________________________________________________________________________________________________
batch_normalization_138 (BatchN (None, 17, 17, 192)  576         conv2d_138[0][0]                 
__________________________________________________________________________________________________
batch_normalization_143 (BatchN (None, 17, 17, 192)  576         conv2d_143[0][0]                 
__________________________________________________________________________________________________
batch_normalization_144 (BatchN (None, 17, 17, 192)  576         conv2d_144[0][0]                 
__________________________________________________________________________________________________
activation_135 (Activation)     (None, 17, 17, 192)  0           batch_normalization_135[0][0]    
__________________________________________________________________________________________________
activation_138 (Activation)     (None, 17, 17, 192)  0           batch_normalization_138[0][0]    
__________________________________________________________________________________________________
activation_143 (Activation)     (None, 17, 17, 192)  0           batch_normalization_143[0][0]    
__________________________________________________________________________________________________
activation_144 (Activation)     (None, 17, 17, 192)  0           batch_normalization_144[0][0]    
__________________________________________________________________________________________________
mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_135[0][0]             
                                                                 activation_138[0][0]             
                                                                 activation_143[0][0]             
                                                                 activation_144[0][0]             
__________________________________________________________________________________________________
conv2d_149 (Conv2D)             (None, 17, 17, 160)  122880      mixed5[0][0]                     
__________________________________________________________________________________________________
batch_normalization_149 (BatchN (None, 17, 17, 160)  480         conv2d_149[0][0]                 
__________________________________________________________________________________________________
activation_149 (Activation)     (None, 17, 17, 160)  0           batch_normalization_149[0][0]    
__________________________________________________________________________________________________
conv2d_150 (Conv2D)             (None, 17, 17, 160)  179200      activation_149[0][0]             
__________________________________________________________________________________________________
batch_normalization_150 (BatchN (None, 17, 17, 160)  480         conv2d_150[0][0]                 
__________________________________________________________________________________________________
activation_150 (Activation)     (None, 17, 17, 160)  0           batch_normalization_150[0][0]    
__________________________________________________________________________________________________
conv2d_146 (Conv2D)             (None, 17, 17, 160)  122880      mixed5[0][0]                     
__________________________________________________________________________________________________
conv2d_151 (Conv2D)             (None, 17, 17, 160)  179200      activation_150[0][0]             
__________________________________________________________________________________________________
batch_normalization_146 (BatchN (None, 17, 17, 160)  480         conv2d_146[0][0]                 
__________________________________________________________________________________________________
batch_normalization_151 (BatchN (None, 17, 17, 160)  480         conv2d_151[0][0]                 
__________________________________________________________________________________________________
activation_146 (Activation)     (None, 17, 17, 160)  0           batch_normalization_146[0][0]    
__________________________________________________________________________________________________
activation_151 (Activation)     (None, 17, 17, 160)  0           batch_normalization_151[0][0]    
__________________________________________________________________________________________________
conv2d_147 (Conv2D)             (None, 17, 17, 160)  179200      activation_146[0][0]             
__________________________________________________________________________________________________
conv2d_152 (Conv2D)             (None, 17, 17, 160)  179200      activation_151[0][0]             
__________________________________________________________________________________________________
batch_normalization_147 (BatchN (None, 17, 17, 160)  480         conv2d_147[0][0]                 
__________________________________________________________________________________________________
batch_normalization_152 (BatchN (None, 17, 17, 160)  480         conv2d_152[0][0]                 
__________________________________________________________________________________________________
activation_147 (Activation)     (None, 17, 17, 160)  0           batch_normalization_147[0][0]    
__________________________________________________________________________________________________
activation_152 (Activation)     (None, 17, 17, 160)  0           batch_normalization_152[0][0]    
__________________________________________________________________________________________________
average_pooling2d_15 (AveragePo (None, 17, 17, 768)  0           mixed5[0][0]                     
__________________________________________________________________________________________________
conv2d_145 (Conv2D)             (None, 17, 17, 192)  147456      mixed5[0][0]                     
__________________________________________________________________________________________________
conv2d_148 (Conv2D)             (None, 17, 17, 192)  215040      activation_147[0][0]             
__________________________________________________________________________________________________
conv2d_153 (Conv2D)             (None, 17, 17, 192)  215040      activation_152[0][0]             
__________________________________________________________________________________________________
conv2d_154 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_15[0][0]       
__________________________________________________________________________________________________
batch_normalization_145 (BatchN (None, 17, 17, 192)  576         conv2d_145[0][0]                 
__________________________________________________________________________________________________
batch_normalization_148 (BatchN (None, 17, 17, 192)  576         conv2d_148[0][0]                 
__________________________________________________________________________________________________
batch_normalization_153 (BatchN (None, 17, 17, 192)  576         conv2d_153[0][0]                 
__________________________________________________________________________________________________
batch_normalization_154 (BatchN (None, 17, 17, 192)  576         conv2d_154[0][0]                 
__________________________________________________________________________________________________
activation_145 (Activation)     (None, 17, 17, 192)  0           batch_normalization_145[0][0]    
__________________________________________________________________________________________________
activation_148 (Activation)     (None, 17, 17, 192)  0           batch_normalization_148[0][0]    
__________________________________________________________________________________________________
activation_153 (Activation)     (None, 17, 17, 192)  0           batch_normalization_153[0][0]    
__________________________________________________________________________________________________
activation_154 (Activation)     (None, 17, 17, 192)  0           batch_normalization_154[0][0]    
__________________________________________________________________________________________________
mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_145[0][0]             
                                                                 activation_148[0][0]             
                                                                 activation_153[0][0]             
                                                                 activation_154[0][0]             
__________________________________________________________________________________________________
conv2d_159 (Conv2D)             (None, 17, 17, 192)  147456      mixed6[0][0]                     
__________________________________________________________________________________________________
batch_normalization_159 (BatchN (None, 17, 17, 192)  576         conv2d_159[0][0]                 
__________________________________________________________________________________________________
activation_159 (Activation)     (None, 17, 17, 192)  0           batch_normalization_159[0][0]    
__________________________________________________________________________________________________
conv2d_160 (Conv2D)             (None, 17, 17, 192)  258048      activation_159[0][0]             
__________________________________________________________________________________________________
batch_normalization_160 (BatchN (None, 17, 17, 192)  576         conv2d_160[0][0]                 
__________________________________________________________________________________________________
activation_160 (Activation)     (None, 17, 17, 192)  0           batch_normalization_160[0][0]    
__________________________________________________________________________________________________
conv2d_156 (Conv2D)             (None, 17, 17, 192)  147456      mixed6[0][0]                     
__________________________________________________________________________________________________
conv2d_161 (Conv2D)             (None, 17, 17, 192)  258048      activation_160[0][0]             
__________________________________________________________________________________________________
batch_normalization_156 (BatchN (None, 17, 17, 192)  576         conv2d_156[0][0]                 
__________________________________________________________________________________________________
batch_normalization_161 (BatchN (None, 17, 17, 192)  576         conv2d_161[0][0]                 
__________________________________________________________________________________________________
activation_156 (Activation)     (None, 17, 17, 192)  0           batch_normalization_156[0][0]    
__________________________________________________________________________________________________
activation_161 (Activation)     (None, 17, 17, 192)  0           batch_normalization_161[0][0]    
__________________________________________________________________________________________________
conv2d_157 (Conv2D)             (None, 17, 17, 192)  258048      activation_156[0][0]             
__________________________________________________________________________________________________
conv2d_162 (Conv2D)             (None, 17, 17, 192)  258048      activation_161[0][0]             
__________________________________________________________________________________________________
batch_normalization_157 (BatchN (None, 17, 17, 192)  576         conv2d_157[0][0]                 
__________________________________________________________________________________________________
batch_normalization_162 (BatchN (None, 17, 17, 192)  576         conv2d_162[0][0]                 
__________________________________________________________________________________________________
activation_157 (Activation)     (None, 17, 17, 192)  0           batch_normalization_157[0][0]    
__________________________________________________________________________________________________
activation_162 (Activation)     (None, 17, 17, 192)  0           batch_normalization_162[0][0]    
__________________________________________________________________________________________________
average_pooling2d_16 (AveragePo (None, 17, 17, 768)  0           mixed6[0][0]                     
__________________________________________________________________________________________________
conv2d_155 (Conv2D)             (None, 17, 17, 192)  147456      mixed6[0][0]                     
__________________________________________________________________________________________________
conv2d_158 (Conv2D)             (None, 17, 17, 192)  258048      activation_157[0][0]             
__________________________________________________________________________________________________
conv2d_163 (Conv2D)             (None, 17, 17, 192)  258048      activation_162[0][0]             
__________________________________________________________________________________________________
conv2d_164 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_16[0][0]       
__________________________________________________________________________________________________
batch_normalization_155 (BatchN (None, 17, 17, 192)  576         conv2d_155[0][0]                 
__________________________________________________________________________________________________
batch_normalization_158 (BatchN (None, 17, 17, 192)  576         conv2d_158[0][0]                 
__________________________________________________________________________________________________
batch_normalization_163 (BatchN (None, 17, 17, 192)  576         conv2d_163[0][0]                 
__________________________________________________________________________________________________
batch_normalization_164 (BatchN (None, 17, 17, 192)  576         conv2d_164[0][0]                 
__________________________________________________________________________________________________
activation_155 (Activation)     (None, 17, 17, 192)  0           batch_normalization_155[0][0]    
__________________________________________________________________________________________________
activation_158 (Activation)     (None, 17, 17, 192)  0           batch_normalization_158[0][0]    
__________________________________________________________________________________________________
activation_163 (Activation)     (None, 17, 17, 192)  0           batch_normalization_163[0][0]    
__________________________________________________________________________________________________
activation_164 (Activation)     (None, 17, 17, 192)  0           batch_normalization_164[0][0]    
__________________________________________________________________________________________________
mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_155[0][0]             
                                                                 activation_158[0][0]             
                                                                 activation_163[0][0]             
                                                                 activation_164[0][0]             
__________________________________________________________________________________________________
conv2d_167 (Conv2D)             (None, 17, 17, 192)  147456      mixed7[0][0]                     
__________________________________________________________________________________________________
batch_normalization_167 (BatchN (None, 17, 17, 192)  576         conv2d_167[0][0]                 
__________________________________________________________________________________________________
activation_167 (Activation)     (None, 17, 17, 192)  0           batch_normalization_167[0][0]    
__________________________________________________________________________________________________
conv2d_168 (Conv2D)             (None, 17, 17, 192)  258048      activation_167[0][0]             
__________________________________________________________________________________________________
batch_normalization_168 (BatchN (None, 17, 17, 192)  576         conv2d_168[0][0]                 
__________________________________________________________________________________________________
activation_168 (Activation)     (None, 17, 17, 192)  0           batch_normalization_168[0][0]    
__________________________________________________________________________________________________
conv2d_165 (Conv2D)             (None, 17, 17, 192)  147456      mixed7[0][0]                     
__________________________________________________________________________________________________
conv2d_169 (Conv2D)             (None, 17, 17, 192)  258048      activation_168[0][0]             
__________________________________________________________________________________________________
batch_normalization_165 (BatchN (None, 17, 17, 192)  576         conv2d_165[0][0]                 
__________________________________________________________________________________________________
batch_normalization_169 (BatchN (None, 17, 17, 192)  576         conv2d_169[0][0]                 
__________________________________________________________________________________________________
activation_165 (Activation)     (None, 17, 17, 192)  0           batch_normalization_165[0][0]    
__________________________________________________________________________________________________
activation_169 (Activation)     (None, 17, 17, 192)  0           batch_normalization_169[0][0]    
__________________________________________________________________________________________________
conv2d_166 (Conv2D)             (None, 8, 8, 320)    552960      activation_165[0][0]             
__________________________________________________________________________________________________
conv2d_170 (Conv2D)             (None, 8, 8, 192)    331776      activation_169[0][0]             
__________________________________________________________________________________________________
batch_normalization_166 (BatchN (None, 8, 8, 320)    960         conv2d_166[0][0]                 
__________________________________________________________________________________________________
batch_normalization_170 (BatchN (None, 8, 8, 192)    576         conv2d_170[0][0]                 
__________________________________________________________________________________________________
activation_166 (Activation)     (None, 8, 8, 320)    0           batch_normalization_166[0][0]    
__________________________________________________________________________________________________
activation_170 (Activation)     (None, 8, 8, 192)    0           batch_normalization_170[0][0]    
__________________________________________________________________________________________________
max_pooling2d_8 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]                     
__________________________________________________________________________________________________
mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_166[0][0]             
                                                                 activation_170[0][0]             
                                                                 max_pooling2d_8[0][0]            
__________________________________________________________________________________________________
conv2d_175 (Conv2D)             (None, 8, 8, 448)    573440      mixed8[0][0]                     
__________________________________________________________________________________________________
batch_normalization_175 (BatchN (None, 8, 8, 448)    1344        conv2d_175[0][0]                 
__________________________________________________________________________________________________
activation_175 (Activation)     (None, 8, 8, 448)    0           batch_normalization_175[0][0]    
__________________________________________________________________________________________________
conv2d_172 (Conv2D)             (None, 8, 8, 384)    491520      mixed8[0][0]                     
__________________________________________________________________________________________________
conv2d_176 (Conv2D)             (None, 8, 8, 384)    1548288     activation_175[0][0]             
__________________________________________________________________________________________________
batch_normalization_172 (BatchN (None, 8, 8, 384)    1152        conv2d_172[0][0]                 
__________________________________________________________________________________________________
batch_normalization_176 (BatchN (None, 8, 8, 384)    1152        conv2d_176[0][0]                 
__________________________________________________________________________________________________
activation_172 (Activation)     (None, 8, 8, 384)    0           batch_normalization_172[0][0]    
__________________________________________________________________________________________________
activation_176 (Activation)     (None, 8, 8, 384)    0           batch_normalization_176[0][0]    
__________________________________________________________________________________________________
conv2d_173 (Conv2D)             (None, 8, 8, 384)    442368      activation_172[0][0]             
__________________________________________________________________________________________________
conv2d_174 (Conv2D)             (None, 8, 8, 384)    442368      activation_172[0][0]             
__________________________________________________________________________________________________
conv2d_177 (Conv2D)             (None, 8, 8, 384)    442368      activation_176[0][0]             
__________________________________________________________________________________________________
conv2d_178 (Conv2D)             (None, 8, 8, 384)    442368      activation_176[0][0]             
__________________________________________________________________________________________________
average_pooling2d_17 (AveragePo (None, 8, 8, 1280)   0           mixed8[0][0]                     
__________________________________________________________________________________________________
conv2d_171 (Conv2D)             (None, 8, 8, 320)    409600      mixed8[0][0]                     
__________________________________________________________________________________________________
batch_normalization_173 (BatchN (None, 8, 8, 384)    1152        conv2d_173[0][0]                 
__________________________________________________________________________________________________
batch_normalization_174 (BatchN (None, 8, 8, 384)    1152        conv2d_174[0][0]                 
__________________________________________________________________________________________________
batch_normalization_177 (BatchN (None, 8, 8, 384)    1152        conv2d_177[0][0]                 
__________________________________________________________________________________________________
batch_normalization_178 (BatchN (None, 8, 8, 384)    1152        conv2d_178[0][0]                 
__________________________________________________________________________________________________
conv2d_179 (Conv2D)             (None, 8, 8, 192)    245760      average_pooling2d_17[0][0]       
__________________________________________________________________________________________________
batch_normalization_171 (BatchN (None, 8, 8, 320)    960         conv2d_171[0][0]                 
__________________________________________________________________________________________________
activation_173 (Activation)     (None, 8, 8, 384)    0           batch_normalization_173[0][0]    
__________________________________________________________________________________________________
activation_174 (Activation)     (None, 8, 8, 384)    0           batch_normalization_174[0][0]    
__________________________________________________________________________________________________
activation_177 (Activation)     (None, 8, 8, 384)    0           batch_normalization_177[0][0]    
__________________________________________________________________________________________________
activation_178 (Activation)     (None, 8, 8, 384)    0           batch_normalization_178[0][0]    
__________________________________________________________________________________________________
batch_normalization_179 (BatchN (None, 8, 8, 192)    576         conv2d_179[0][0]                 
__________________________________________________________________________________________________
activation_171 (Activation)     (None, 8, 8, 320)    0           batch_normalization_171[0][0]    
__________________________________________________________________________________________________
mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_173[0][0]             
                                                                 activation_174[0][0]             
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 8, 8, 768)    0           activation_177[0][0]             
                                                                 activation_178[0][0]             
__________________________________________________________________________________________________
activation_179 (Activation)     (None, 8, 8, 192)    0           batch_normalization_179[0][0]    
__________________________________________________________________________________________________
mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_171[0][0]             
                                                                 mixed9_0[0][0]                   
                                                                 concatenate_3[0][0]              
                                                                 activation_179[0][0]             
__________________________________________________________________________________________________
conv2d_184 (Conv2D)             (None, 8, 8, 448)    917504      mixed9[0][0]                     
__________________________________________________________________________________________________
batch_normalization_184 (BatchN (None, 8, 8, 448)    1344        conv2d_184[0][0]                 
__________________________________________________________________________________________________
activation_184 (Activation)     (None, 8, 8, 448)    0           batch_normalization_184[0][0]    
__________________________________________________________________________________________________
conv2d_181 (Conv2D)             (None, 8, 8, 384)    786432      mixed9[0][0]                     
__________________________________________________________________________________________________
conv2d_185 (Conv2D)             (None, 8, 8, 384)    1548288     activation_184[0][0]             
__________________________________________________________________________________________________
batch_normalization_181 (BatchN (None, 8, 8, 384)    1152        conv2d_181[0][0]                 
__________________________________________________________________________________________________
batch_normalization_185 (BatchN (None, 8, 8, 384)    1152        conv2d_185[0][0]                 
__________________________________________________________________________________________________
activation_181 (Activation)     (None, 8, 8, 384)    0           batch_normalization_181[0][0]    
__________________________________________________________________________________________________
activation_185 (Activation)     (None, 8, 8, 384)    0           batch_normalization_185[0][0]    
__________________________________________________________________________________________________
conv2d_182 (Conv2D)             (None, 8, 8, 384)    442368      activation_181[0][0]             
__________________________________________________________________________________________________
conv2d_183 (Conv2D)             (None, 8, 8, 384)    442368      activation_181[0][0]             
__________________________________________________________________________________________________
conv2d_186 (Conv2D)             (None, 8, 8, 384)    442368      activation_185[0][0]             
__________________________________________________________________________________________________
conv2d_187 (Conv2D)             (None, 8, 8, 384)    442368      activation_185[0][0]             
__________________________________________________________________________________________________
average_pooling2d_18 (AveragePo (None, 8, 8, 2048)   0           mixed9[0][0]                     
__________________________________________________________________________________________________
conv2d_180 (Conv2D)             (None, 8, 8, 320)    655360      mixed9[0][0]                     
__________________________________________________________________________________________________
batch_normalization_182 (BatchN (None, 8, 8, 384)    1152        conv2d_182[0][0]                 
__________________________________________________________________________________________________
batch_normalization_183 (BatchN (None, 8, 8, 384)    1152        conv2d_183[0][0]                 
__________________________________________________________________________________________________
batch_normalization_186 (BatchN (None, 8, 8, 384)    1152        conv2d_186[0][0]                 
__________________________________________________________________________________________________
batch_normalization_187 (BatchN (None, 8, 8, 384)    1152        conv2d_187[0][0]                 
__________________________________________________________________________________________________
conv2d_188 (Conv2D)             (None, 8, 8, 192)    393216      average_pooling2d_18[0][0]       
__________________________________________________________________________________________________
batch_normalization_180 (BatchN (None, 8, 8, 320)    960         conv2d_180[0][0]                 
__________________________________________________________________________________________________
activation_182 (Activation)     (None, 8, 8, 384)    0           batch_normalization_182[0][0]    
__________________________________________________________________________________________________
activation_183 (Activation)     (None, 8, 8, 384)    0           batch_normalization_183[0][0]    
__________________________________________________________________________________________________
activation_186 (Activation)     (None, 8, 8, 384)    0           batch_normalization_186[0][0]    
__________________________________________________________________________________________________
activation_187 (Activation)     (None, 8, 8, 384)    0           batch_normalization_187[0][0]    
__________________________________________________________________________________________________
batch_normalization_188 (BatchN (None, 8, 8, 192)    576         conv2d_188[0][0]                 
__________________________________________________________________________________________________
activation_180 (Activation)     (None, 8, 8, 320)    0           batch_normalization_180[0][0]    
__________________________________________________________________________________________________
mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_182[0][0]             
                                                                 activation_183[0][0]             
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 8, 8, 768)    0           activation_186[0][0]             
                                                                 activation_187[0][0]             
__________________________________________________________________________________________________
activation_188 (Activation)     (None, 8, 8, 192)    0           batch_normalization_188[0][0]    
__________________________________________________________________________________________________
mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_180[0][0]             
                                                                 mixed9_1[0][0]                   
                                                                 concatenate_4[0][0]              
                                                                 activation_188[0][0]             
__________________________________________________________________________________________________
global_average_pooling2d_2 (Glo (None, 2048)         0           mixed10[0][0]                    
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1024)         2098176     global_average_pooling2d_2[0][0] 
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 10)           10250       dense_3[0][0]                    
==================================================================================================
Total params: 23,911,210
Trainable params: 13,223,306
Non-trainable params: 10,687,904
__________________________________________________________________________________________________
#Start Training top layers
inception_layers_file_path="inception_layers.iv3.hdf5"
checkpoint = ModelCheckpoint(inception_layers_file_path, monitor='loss', verbose=1, save_best_only=True, mode='min')
​
train_flow.reset()
val_flow.reset()
history = model.fit_generator(train_flow, 
                              epochs=5, 
                              verbose=1,
                              validation_data=val_flow,
                              validation_steps=math.ceil(val_flow.samples/val_flow.batch_size),
                              steps_per_epoch=math.ceil(train_flow.samples/train_flow.batch_size),
                              callbacks=[checkpoint, early, tb, csv_logger])
Epoch 1/5
80/80 [==============================] - 111s 1s/step - loss: 0.7765 - acc: 0.7099 - top_k_categorical_accuracy: 0.9869 - val_loss: 0.5509 - val_acc: 0.8003 - val_top_k_categorical_accuracy: 0.9949

Epoch 00001: loss improved from inf to 0.77821, saving model to inception_layers.iv3.hdf5
Epoch 2/5
80/80 [==============================] - 103s 1s/step - loss: 0.2500 - acc: 0.9232 - top_k_categorical_accuracy: 0.9996 - val_loss: 0.3808 - val_acc: 0.8584 - val_top_k_categorical_accuracy: 0.9949

Epoch 00002: loss improved from 0.77821 to 0.25018, saving model to inception_layers.iv3.hdf5
Epoch 3/5
80/80 [==============================] - 103s 1s/step - loss: 0.1555 - acc: 0.9620 - top_k_categorical_accuracy: 0.9997 - val_loss: 0.4497 - val_acc: 0.8379 - val_top_k_categorical_accuracy: 0.9949

Epoch 00003: loss improved from 0.25018 to 0.15480, saving model to inception_layers.iv3.hdf5
Epoch 4/5
80/80 [==============================] - 103s 1s/step - loss: 0.2075 - acc: 0.9435 - top_k_categorical_accuracy: 0.9998 - val_loss: 0.4170 - val_acc: 0.8720 - val_top_k_categorical_accuracy: 0.9949

Epoch 00004: loss did not improve from 0.15480
Epoch 5/5
80/80 [==============================] - 103s 1s/step - loss: 0.3146 - acc: 0.9188 - top_k_categorical_accuracy: 0.9994 - val_loss: 0.5128 - val_acc: 0.8549 - val_top_k_categorical_accuracy: 0.9983

Epoch 00005: loss did not improve from 0.15480
#Load Trained Model and Test
model.load_weights(inception_layers_file_path)
test_flow.reset()
loss, acc, top_5 = model.evaluate_generator(
    test_flow,
    verbose = True,
    steps=math.ceil(test_flow.samples/test_flow.batch_size))
print("Loss: ", loss)
print("Acc: ", acc)
print("Top 5: ", top_5)
19/19 [==============================] - 35s 2s/step
Loss:  0.4909451117259759
Acc:  0.8357082986571863
Top 5:  0.9949706621961442
file_list = glob.glob("test/*/*")
img_path = random.choice(file_list)
img_cat = os.path.split(os.path.dirname(img_path))[1]
print("Image Category: ", img_cat)
img = image.load_img(img_path, target_size=(299, 299))
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x = preprocess_input(x)
​
preds = model.predict(x)
print("Raw Predictions: ", preds)
​
top_x = 3
top_args = preds[0].argsort()[-top_x:][::-1]
preds_label = [label[p] for p in top_args]
print("\nTop " + str(top_x) + " confidence: " + " ".join(map(str, sorted(preds[0])[-top_x:][::-1])))
print("Top " + str(top_x) + " labels: " + " ".join(map(str, preds_label)))
Image Category:  dodge_ram_2001
Raw Predictions:  [[2.8266115e-08 9.4401234e-01 5.5080686e-02 1.7939497e-05 8.8671345e-04
  3.9746612e-07 1.9541294e-06 3.1669714e-08 5.0141558e-09 1.1208988e-08]]

Top 3 confidence: 0.94401234 0.055080686 0.00088671345
Top 3 labels: chevrolet_silverado_2004 dodge_ram_2001 gmc_sierra_2012
from tensorflow.python.framework import graph_util
from tensorflow.python.framework import graph_io
​
input_model_path = inception_layers_file_path
output_model_name = "inception_layers.iv3.pb"
output_model_dir = "tf_model"
​
K.set_learning_phase(0)
sess = K.get_session()
​
test_model = models.load_model(input_model_path)
orig_output_node_names = [node.op.name for node in test_model.outputs]
​
constant_graph = graph_util.convert_variables_to_constants(
    sess,
    sess.graph.as_graph_def(),
    orig_output_node_names)
graph_io.write_graph(
    constant_graph,
    output_model_dir,
    output_model_name,
    as_text=False)
INFO:tensorflow:Froze 380 variables.
INFO:tensorflow:Converted 380 variables to const ops.
'tf_model/inception_layers.iv3.pb'
Resources
TensorFlow* Optimizations on Modern Intel® Architecture, https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture

Intel Optimized TensorFlow Wheel Now Available, https://software.intel.com/en-us/articles/intel-optimized-tensorflow-wheel-now-available

Build and Install TensorFlow* on Intel® Architecture, https://software.intel.com/en-us/articles/build-and-install-tensorflow-on-intel-architecture

TensorFlow, https://www.tensorflow.org/

Case Studies
Manufacturing Package Fault Detection Using Deep Learning, https://software.intel.com/en-us/articles/manufacturing-package-fault-detection-using-deep-learning

Automatic Defect Inspection Using Deep Learning for Solar Farm, https://software.intel.com/en-us/articles/automatic-defect-inspection-using-deep-learning-for-solar-farm


